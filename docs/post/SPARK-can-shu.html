<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />
    
    <link rel="icon" href="https://github.githubassets.com/favicons/favicon.svg"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="

## Driver

**spark.driver.cores**

driver端分配的核数，默认为1，thriftserver是启动thriftserver服务的机器，资源充足的话可以尽量给多。">
<meta property="og:title" content="SPARK参数">
<meta property="og:description" content="

## Driver

**spark.driver.cores**

driver端分配的核数，默认为1，thriftserver是启动thriftserver服务的机器，资源充足的话可以尽量给多。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://0or1world.github.io/post/SPARK-can-shu.html">
<meta property="og:image" content="https://github.githubassets.com/favicons/favicon.svg">
<title>SPARK参数</title>



</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}
</style>




<body>
    <div id="header">
<h1 class="postTitle">SPARK参数</h1>
<div class="title-right">
    <a href="https://0or1world.github.io" id="buttonHome" class="btn btn-invisible circle" title="首页">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/0or1world/0or1world.github.io/issues/2" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="切换主题">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><h2>Driver</h2>
<p><strong>spark.driver.cores</strong></p>
<p>driver端分配的核数，默认为1，thriftserver是启动thriftserver服务的机器，资源充足的话可以尽量给多。</p>
<p><strong>spark.driver.memory</strong></p>
<p>driver端分配的内存数，默认为1g，同上。</p>
<p><strong>spark.driver.maxResultSize</strong></p>
<p>driver端接收的最大结果大小，默认1GB，最小1MB，设置0为无限。<br>
这个参数不建议设置的太大，如果要做数据可视化，更应该控制在20-30MB以内。过大会导致OOM。</p>
<p><strong>spark.extraListeners</strong></p>
<p>默认none，随着SparkContext被创建而创建，用于监听单参数、无参数构造函数的创建，并抛出异常。</p>
<h2>Executor</h2>
<p><strong>spark.executor.memory</strong><br>
每个executor分配的内存数，默认1g，会受到yarn CDH的限制，和memoryOverhead相加 不能超过总内存限制。</p>
<p><strong>spark.executor.cores</strong></p>
<p>每个executor的核数，默认yarn下1核，standalone下为所有可用的核。</p>
<p><strong>spark.default.parallelism</strong></p>
<p>默认RDD的分区数、并行数。<br>
像reduceByKey和join等这种需要分布式shuffle的操作中，最大父RDD的分区数；像parallelize之类没有父RDD的操作，则取决于运行环境下得cluster manager：<br>
如果为单机模式，本机核数；集群模式为所有executor总核数与2中最大的一个。</p>
<p><strong>spark.executor.heartbeatInterval</strong></p>
<p>executor和driver心跳发送间隔，默认10s，必须远远小于spark.network.timeout</p>
<p><strong>spark.files.fetchTimeout</strong></p>
<p>从driver端执行SparkContext.addFile() 抓取添加的文件的超时时间，默认60s</p>
<p><strong>spark.files.useFetchCache</strong></p>
<p>默认true，如果设为true，拉取文件时会在同一个application中本地持久化，被若干个executors共享。这使得当同一个主机下有多个executors时，执行任务效率提高。</p>
<p><strong>spark.broadcast.blockSize</strong></p>
<p>TorrentBroadcastFactory中的每一个block大小，默认4m<br>
过大会减少广播时的并行度，过小会导致BlockManager 产生 performance hit.</p>
<p><strong>spark.files.overwrite</strong></p>
<p>默认false，是否在执行SparkContext.addFile() 添加文件时，覆盖已有的内容有差异的文件。</p>
<p><strong>spark.files.maxPartitionBytes</strong></p>
<p>单partition中最多能容纳的文件大小，单位Bytes 默认134217728 (128 MB)</p>
<p><strong>spark.files.openCostInBytes</strong></p>
<p>小文件合并阈值，小于该参数就会被合并到一个partition内。<br>
默认4194304 (4 MB) 。这个参数在将多个文件放入一个partition时被用到，宁可设置的小一些，因为在partition操作中，小文件肯定会比大文件快。</p>
<p><strong>spark.storage.memoryMapThreshold</strong></p>
<p>从磁盘上读文件时，最小单位不能少于该设定值，默认2m，小于或者接近操作系统的每个page的大小。</p>
<h2>Shuffle</h2>
<p><strong>spark.reducer.maxSizeInFlight</strong></p>
<p>默认48m。从每个reduce任务同时拉取的最大map数，每个reduce都会在完成任务后，需要一个堆外内存的缓冲区来存放结果，如果没有充裕的内存就尽可能把这个调小一点。。相反，堆外内存充裕，调大些就能节省gc时间。</p>
<p><strong>spark.reducer.maxBlocksInFlightPerAddress</strong></p>
<p>限制了每个主机每次reduce可以被多少台远程主机拉取文件块，调低这个参数可以有效减轻node manager的负载。（默认值Int.MaxValue）</p>
<p><strong>spark.reducer.maxReqsInFlight</strong></p>
<p>限制远程机器拉取本机器文件块的请求数，随着集群增大，需要对此做出限制。否则可能会使本机负载过大而挂掉。。（默认值为Int.MaxValue）</p>
<p><strong>spark.reducer.maxReqSizeShuffleToMem</strong></p>
<p>shuffle请求的文件块大小 超过这个参数值，就会被强行落盘，防止一大堆并发请求把内存占满。（默认Long.MaxValue）</p>
<p><strong>spark.shuffle.compress</strong></p>
<p>是否压缩map输出文件，默认压缩 true</p>
<p><strong>spark.shuffle.spill.compress</strong></p>
<p>shuffle过程中溢出的文件是否压缩，默认true，使用spark.io.compression.codec压缩。</p>
<p><strong>spark.shuffle.file.buffer</strong></p>
<p>在内存输出流中 每个shuffle文件占用内存大小，适当提高 可以减少磁盘读写 io次数，初始值为32k</p>
<p><strong>spark.shuffle.memoryFraction</strong></p>
<p>该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。<br>
cache少且内存充足时，可以调大该参数，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。</p>
<p><strong>spark.shuffle.manager</strong></p>
<p>当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。</p>
<p>当使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。</p>
<p><strong>spark.shuffle.consolidateFiles</strong></p>
<p>如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。</p>
<p>如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shuffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。</p>
<p><strong>spark.shuffle.io.maxRetries</strong></p>
<p>shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。</p>
<p>对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。</p>
<p><strong>spark.shuffle.io.retryWait</strong></p>
<p>同上，默认5s，建议加大间隔时长（比如60s），以增加shuffle操作的稳定性</p>
<h2>Compression and Serialization</h2>
<p><strong>spark.broadcast.compress</strong></p>
<p>广播变量前是否会先进行压缩。默认true （spark.io.compression.codec）</p>
<p><strong>spark.io.compression.codec</strong></p>
<p>压缩RDD数据、日志、shuffle输出等的压缩格式 默认lz4</p>
<p><strong>spark.io.compression.lz4.blockSize</strong></p>
<p>使用lz4压缩时，每个数据块大小 默认32k</p>
<p><strong>spark.rdd.compress</strong></p>
<p>rdd是否压缩 默认false，节省memory_cache大量内存 消耗更多的cpu资源（时间）。</p>
<p><strong>spark.serializer.objectStreamReset</strong></p>
<p>当使用JavaSerializer序列化时，会缓存对象防止写多余的数据，但这些对象就不会被gc，可以输入reset 清空缓存。默认缓存100个对象，修改成-1则不缓存任何对象。</p>
<h2>Memory Management</h2>
<p><strong>spark.memory.fraction</strong></p>
<p>执行内存和缓存内存（堆）占jvm总内存的比例，剩余的部分是spark留给用户存储内部源数据、数据结构、异常大的结果数据。<br>
默认值0.6，调小会导致频繁gc，调大容易造成oom。</p>
<p><strong>spark.memory.storageFraction</strong></p>
<p>用于存储的内存在堆中的占比，默认0.5。调大会导致执行内存过小，执行数据落盘，影响效率；调小会导致缓存内存不够，缓存到磁盘上去，影响效率。</p>
<p>值得一提的是在spark中，执行内存和缓存内存公用java堆，当执行内存没有使用时，会动态分配给缓存内存使用，反之也是这样。如果执行内存不够用，可以将存储内存释放移动到磁盘上（最多释放不能超过本参数划分的比例），但存储内存不能把执行内存抢走。</p>
<p><strong>spark.memory.offHeap.enabled</strong></p>
<p>是否允许使用堆外内存来进行某些操作。默认false</p>
<p><strong>spark.memory.offHeap.size</strong></p>
<p>允许使用进行操作的堆外内存的大小，单位bytes 默认0</p>
<p><strong>spark.cleaner.periodicGC.interval</strong></p>
<p>控制触发gc的频率，默认30min</p>
<p><strong>spark.cleaner.referenceTracking</strong></p>
<p>是否进行context cleaning，默认true</p>
<p><strong>spark.cleaner.referenceTracking.blocking</strong></p>
<p>清理线程是否应该阻止清理任务，默认true</p>
<p><strong>spark.cleaner.referenceTracking.blocking.shuffle</strong></p>
<p>清理线程是否应该阻止shuffle的清理任务，默认false</p>
<p><strong>spark.cleaner.referenceTracking.cleanCheckpoints</strong></p>
<p>清理线程是否应该清理依赖超出范围的检查点文件（checkpoint files不知道怎么翻译。。）默认false</p>
<h2>Networking</h2>
<p><strong>spark.rpc.message.maxSize</strong></p>
<p>executors和driver间消息传输、map输出的大小，默认128M。map多可以考虑增加。</p>
<p><strong>spark.driver.blockManager.port和spark.driver.bindAddress</strong></p>
<p>driver端绑定监听block manager的地址与端口。</p>
<p><strong>spark.driver.host和spark.driver.port</strong></p>
<p>driver端的ip和端口。</p>
<p><strong>spark.network.timeout</strong></p>
<p>网络交互超时时间，默认120s。如果<br>
spark.core.connection.ack.wait.timeout<br>
spark.storage.blockManagerSlaveTimeoutMs<br>
spark.shuffle.io.connectionTimeout<br>
spark.rpc.askTimeout orspark.rpc.lookupTimeout<br>
没有设置，那么就以此参数为准。</p>
<p><strong>spark.port.maxRetries</strong></p>
<p>设定了一个端口后，在放弃之前的最大重试次数，默认16。 会有一个预重试机制，每次会尝试前一次尝试的端口号+1的端口。如 设定了端口为8000，则最终会尝试8000~(8000+16)范围的端口。</p>
<p><strong>spark.rpc.numRetries</strong></p>
<p>rpc任务在放弃之前的重试次数，默认3，即rpc task最多会执行3次。</p>
<p><strong>spark.rpc.retry.wait</strong></p>
<p>重试间隔，默认3s</p>
<p><strong>spark.rpc.askTimeout</strong></p>
<p>rpc任务超时时间，默认spark.network.timeout</p>
<p><strong>spark.rpc.lookupTimeout</strong></p>
<p>rpc任务查找时长</p>
<h2>Scheduling</h2>
<p><strong>spark.scheduler.maxRegisteredResourcesWaitingTime</strong></p>
<p>在执行前最大等待申请资源的时间，默认30s。</p>
<p><strong>spark.scheduler.minRegisteredResourcesRatio</strong></p>
<p>实际注册的资源数占预期需要的资源数的比例，默认0.8</p>
<p><strong>spark.scheduler.mode</strong></p>
<p>调度模式，默认FIFO 先进队列先调度，可以选择FAIR。</p>
<p><strong>spark.scheduler.revive.interval</strong></p>
<p>work回复重启的时间间隔，默认1s</p>
<p><strong>spark.scheduler.listenerbus.eventqueue.capacity</strong></p>
<p>spark事件监听队列容量，默认10000，必须为正值，增加可能会消耗更多内存</p>
<p><strong>spark.blacklist.enabled</strong></p>
<p>是否列入黑名单，默认false。如果设成true，当一个executor失败好几次时，会被列入黑名单，防止后续task派发到这个executor。可以进一步调节spark.blacklist以下相关的参数：<br>
（均为测试参数 Experimental）<br>
spark.blacklist.timeout<br>
spark.blacklist.task.maxTaskAttemptsPerExecutor<br>
spark.blacklist.task.maxTaskAttemptsPerNode<br>
spark.blacklist.stage.maxFailedTasksPerExecutor<br>
spark.blacklist.application.maxFailedExecutorsPerNode<br>
spark.blacklist.killBlacklistedExecutors<br>
spark.blacklist.application.fetchFailure.enabled</p>
<p><strong>spark.speculation</strong></p>
<p>推测，如果有task执行的慢了，就会重新执行它。默认false，</p>
<p>详细相关配置如下：<br>
<strong>spark.speculation.interval</strong></p>
<p>检查task快慢的频率，推测间隔，默认100ms。</p>
<p><strong>spark.speculation.multiplier</strong></p>
<p>推测比均值慢几次算是task执行过慢，默认1.5</p>
<p><strong>spark.speculation.quantile</strong></p>
<p>在某个stage，完成度必须达到该参数的比例，才能被推测，默认0.75</p>
<p><strong>spark.task.cpus</strong></p>
<p>每个task分配的cpu数，默认1</p>
<p><strong>spark.task.maxFailures</strong></p>
<p>在放弃这个job前允许的最大失败次数，重试次数为该参数-1，默认4</p>
<p><strong>spark.task.reaper.enabled</strong></p>
<p>赋予spark监控有权限去kill那些失效的task，默认false<br>
(原先有 job失败了但一直显示有task在running，总算找到这个参数了)</p>
<p>其他进阶的配置如下：<br>
<strong>spark.task.reaper.pollingInterval</strong></p>
<p>轮询被kill掉的task的时间间隔，如果还在running，就会打warn日志，默认10s。</p>
<p><strong>spark.task.reaper.threadDump</strong></p>
<p>线程回收是是否产生日志，默认true。</p>
<p><strong>spark.task.reaper.killTimeout</strong></p>
<p>当一个被kill的task过了多久还在running，就会把那个executor给kill掉，默认-1。</p>
<p><strong>spark.stage.maxConsecutiveAttempts</strong></p>
<p>在终止前，一个stage连续尝试次数，默认4。</p>
<h2>Dynamic Allocation</h2>
<p><strong>spark.dynamicAllocation.enabled</strong></p>
<p>是否开启动态资源配置，根据工作负载来衡量是否应该增加或减少executor，默认false</p>
<p>以下相关参数：<br>
<strong>spark.dynamicAllocation.minExecutors</strong></p>
<p>动态分配最小executor个数，在启动时就申请好的，默认0</p>
<p><strong>spark.dynamicAllocation.maxExecutors</strong></p>
<p>动态分配最大executor个数，默认infinity</p>
<p><strong>spark.dynamicAllocation.initialExecutors</strong></p>
<p>动态分配初始executor个数默认值=spark.dynamicAllocation.minExecutors</p>
<p><strong>spark.dynamicAllocation.executorIdleTimeout</strong></p>
<p>当某个executor空闲超过这个设定值，就会被kill，默认60s</p>
<p><strong>spark.dynamicAllocation.cachedExecutorIdleTimeout</strong></p>
<p>当某个缓存数据的executor空闲时间超过这个设定值，就会被kill，默认infinity</p>
<p><strong>spark.dynamicAllocation.schedulerBacklogTimeout</strong></p>
<p>任务队列非空，资源不够，申请executor的时间间隔，默认1s</p>
<p><strong>spark.dynamicAllocation.sustainedSchedulerBacklogTimeout</strong></p>
<p>同schedulerBacklogTimeout，是申请了新executor之后继续申请的间隔，默认=schedulerBacklogTimeout</p>
<h2>Spark Streaming</h2>
<p><strong>spark.streaming.stopGracefullyOnShutdown</strong> （true / false）默认fasle</p>
<p>确保在kill任务时，能够处理完最后一批数据，再关闭程序，不会发生强制kill导致数据处理中断，没处理完的数据丢失</p>
<p><strong>spark.streaming.backpressure.enabled</strong> （true / false） 默认false</p>
<p>开启后spark自动根据系统负载选择最优消费速率</p>
<p><strong>spark.streaming.backpressure.initialRate</strong> （整数） 默认直接读取所有</p>
<p>在开启反压的情况下，限制第一次批处理应该消费的数据，因为程序冷启动队列里面有大量积压，防止第一次全部读取，造成系统阻塞</p>
<p><strong>spark.streaming.kafka.maxRatePerPartition</strong> （整数） 默认直接读取所有</p>
<p>限制每秒每个消费线程读取每个kafka分区最大的数据量</p>
<p><strong>spark.streaming.unpersist</strong></p>
<p>自动将spark streaming产生的、持久化的数据给清理掉，默认true，自动清理内存垃圾。</p>
<p><strong>spark.streaming.ui.retainedBatches</strong><br>
spark streaming 日志接口在gc时保留的batch个数，默认1000</p></div>
<div style="font-size:small;margin-top:8px;float:right;"></div>

<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">评论</button>
<div class="comments" id="comments"></div>

</div>
    <div id="footer"><div id="footer1">Copyright © <span id="copyrightYear"></span> <a href="https://0or1world.github.io">SHIJIE的技术记录</a></div>
<div id="footer2">
    <span id="runday"></span><span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a></span>
</div>

<script>
var now=new Date();
document.getElementById("copyrightYear").innerHTML=now.getFullYear();

if(""!=""){
    var startSite=new Date("");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("runday").innerHTML="网站运行"+diffDay+"天"+" • ";
}
</script></div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);



function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","0or1world/0or1world.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}
</script>


</html>
