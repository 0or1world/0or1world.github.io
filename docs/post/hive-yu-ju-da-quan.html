<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />
    <link rel="icon" href="https://github.githubassets.com/favicons/favicon.svg"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="hive数据库是hdfs上的文件夹，表也是文件夹，表里的数据是文件
hive建表
create table t_student(id string,name string,age int,classNo string)
row format delimited
fields terminated by ',';

创建外部表
create external table t_a(id string,name string)
row format delimited
fields terminated by ','
location '/.../...'
外部表和内部表的区别，drop时内部表的hdfs数据一同删除，外部表的hdfs上的数据不删除

create table t_b(id string,name string)
row format delimited
fields terminated by ','

create table t_a(id string,name string)
row format delimited
fields terminated by ','

--加载数据从hdfs上加载
load data inpath '/dataB/b.txt' into table t_a;
--加载数据从本地上加载
load data local inpath '/root/bb.txt' into table t_b;

t_a
+---------+-----------+--+
| t_a.id | t_a.name |
+---------+-----------+--+
| a | 1 |
| b | 2 |
| c | 4 |
| d | 5 |
+---------+-----------+--+

t_b
+---------+-----------+--+
| t_b.id | t_b.name |
+---------+-----------+--+
| a | xx |
| b | yy |
| d | zz |
| e | pp |
+---------+-----------+--+

--笛卡尔积
select a.,b.
from t_a inner join t_b

+---------+-----------+---------+-----------+--+
| t_a.id | t_a.name | t_b.id | t_b.name |
+---------+-----------+---------+-----------+--+
| a | 1 | a | xx |
| b | 2 | a | xx |
| c | 4 | a | xx |
| d | 5 | a | xx |
| a | 1 | b | yy |
| b | 2 | b | yy |
| c | 4 | b | yy |
| d | 5 | b | yy |
| a | 1 | d | zz |
| b | 2 | d | zz |
| c | 4 | d | zz |
| d | 5 | d | zz |
| a | 1 | e | pp |
| b | 2 | e | pp |
| c | 4 | e | pp |
| d | 5 | e | pp |
+---------+-----------+---------+-----------+--+

--内连接
select * from t_a join t_b where t_a.id = t_b.id;
select * from t_a join t_b on t_a.id = t_b.id;

--左外连接
select
a.,b.
from t_a a left join t_b b

select
a.,b.
from t_a a left join t_b b on a.id = b.id

+-------+---------+-------+---------+--+
| a.id | a.name | b.id | b.name |
+-------+---------+-------+---------+--+
| a | 1 | a | xx |
| b | 2 | b | yy |
| d | 5 | d | zz |
| NULL | NULL | e | pp |
+-------+---------+-------+---------+--+

--右外连接
select
a.,b.
from t_a a right outer join t_b b

select
a.,b.
from t_a a right outer join t_b b on a.id = b.id
+-------+---------+-------+---------+--+
| a.id | a.name | b.id | b.name |
+-------+---------+-------+---------+--+
| a | 1 | a | xx |
| b | 2 | b | yy |
| d | 5 | d | zz |
| NULL | NULL | e | pp |
+-------+---------+-------+---------+--+

--全外连接
select
a.,b.
from t_a a full outer join t_b b on a.id = b.id

+-------+---------+-------+---------+--+
| a.id | a.name | b.id | b.name |
+-------+---------+-------+---------+--+
| a | 1 | a | xx |
| b | 2 | b | yy |
| c | 4 | NULL | NULL |
| d | 5 | d | zz |
| NULL | NULL | e | pp |
+-------+---------+-------+---------+--+

--左半连接 相当于指定条件的内连接，但是只显示左边的表数据
select
a.*
from t_a a left semi join t_b b on a.id = b.id

+-------+---------+--+
| a.id | a.name |
+-------+---------+--+
| a | 1 |
| b | 2 |
| d | 5 |
+-------+---------+--+

--例如有这些数据
vi access.log.0804
192.168.33.3,http://www.sina.com/stu,2017-08-04 15:30:20
192.168.33.3,http://www.sina.com/teach,2017-08-04 15:35:20
192.168.33.4,http://www.sina.com/stu,2017-08-04 15:30:20
192.168.33.4,http://www.sina.com/job,2017-08-04 16:30:20
192.168.33.5,http://www.sina.com/job,2017-08-04 15:40:20

vi access.log.0805
192.168.33.3,http://www.sina.com/stu,2017-08-05 15:30:20
192.168.44.3,http://www.sina.com/teach,2017-08-05 15:35:20
192.168.33.44,http://www.sina.com/stu,2017-08-05 15:30:20
192.168.33.46,http://www.sina.com/job,2017-08-05 16:30:20
192.168.33.55,http://www.sina.com/job,2017-08-05 15:40:20

vi access.log.0806
192.168.133.3,http://www.sina.com/register,2017-08-06 15:30:20
192.168.111.3,http://www.sina.com/register,2017-08-06 15:35:20
192.168.34.44,http://www.sina.com/pay,2017-08-06 15:30:20
192.168.33.46,http://www.sina.com/excersize,2017-08-06 16:30:20
192.168.33.55,http://www.sina.com/job,2017-08-06 15:40:20
192.168.33.46,http://www.sina.com/excersize,2017-08-06 16:30:20
192.168.33.25,http://www.sina.com/job,2017-08-06 15:40:20
192.168.33.36,http://www.sina.com/excersize,2017-08-06 16:30:20
192.168.33.55,http://www.sina.com/job,2017-08-06 15:40:20

--建分区表
create table t_access(ip string,url string,access_time string)
partitioned by (day string)
row format delimited
fields terminated by ','

--导入分区表数据
load data local inpath '/root/access.log.0806' into table t_access partition(day='2017-08-06');

--查看当前分区情况
show partitions t_access;

--求每条url访问的次数
select
url,count(1)
from t_access
group by url;

--求每条url访问的ip中，最大的ip是哪个
select
url,max(ip)
from t_access
group by url;

--求每个ip访问同一个页面的记录中，最晚的一条
select
ip,url,max(access_time)
from t_access
group by ip,url

--求PV
select
url,count(1)
from t_access
group by url

+--------------------------------+------+--+
| url | _c1 |
+--------------------------------+------+--+
| http://www.sina.com/excersize | 3 |
| http://www.sina.com/job | 7 |
| http://www.sina.com/pay | 1 |
| http://www.sina.com/register | 2 |
| http://www.sina.com/stu | 4 |
| http://www.sina.com/teach | 2 |
+--------------------------------+------+--+

--求UV
select
url,count(distinct(ip))
from t_access
group by url

+--------------------------------+-----+--+
| url | c1 |
+--------------------------------+-----+--+
| http://www.sina.com/excersize | 2 |
| http://www.sina.com/job | 5 |
| http://www.sina.com/pay | 1 |
| http://www.sina.com/register | 2 |
| http://www.sina.com/stu | 3 |
| http://www.sina.com/teach | 2 |
+--------------------------------+-----+--+

--求每条url访问的ip中，最大的ip是哪个
--PV
--UV
--用mapreduce实现

--求8月6号pv，uv
select
count(1),url
from t_access
where day = '2017-08-06'
group by url

--求8月4号以后，每天访问http://www.sina.com/job 的次数，以及访问者中ip最大的
select
count(1),max(ip),day
from t_access
where day > '2017-08-04'
and url = 'http://www.sina.com/job'
group by day

select
count(1),max(ip),day
from t_access
where url = 'http://www.sina.com/job'
group by day having day > '2017-08-04'

--求8月4号以后，每天每个页面访问总次数，以及页面最大的ip，并且上述查询结果中访问次数大于2的
select tmp.* from
(select
url,day,count(1) cnts,max(ip) ip
from t_access
group by day,url having day > '2017-08-04') tmp
where tmp.cnts > 2

--每天，pv排序
select
url,day,count(1) cnts
from t_access
group by day,url
order by cnts desc

--hive里的order by是全局排序
--sort by是在map里进行排序

select
tmp.*
from
(select
url,day,count(1) cnts
from t_access
group by day,url) tmp
distribute by tmp.day sort by tmp.cnts desc

--有如下数据
+-------------+---------------+--+
| t_sale.mid | t_sale.money |
+-------------+---------------+--+
| AA | 15.0 |
| AA | 20.0 |
| BB | 22.0 |
| CC | 44.0 |
+-------------+---------------+--+

--distribute by先把数据分发到各个reduce中，然后sort by在各个reduce中进行局部排序
select
mid,money
from t_sale
distribute by mid sort by money desc

+------+--------+--+
| mid | money |
+------+--------+--+
| AA | 15.0 |
| AA | 20.0 |
| BB | 22.0 |
| CC | 44.0 |
+------+--------+--+

--cluster by mid 等于 distribute by mid sort by mid
--cluster by后面不能跟desc，asc，默认的只能升序

--order by 是全排序，所有的数据会发送给一个reduceTask进行处理，在数据量大的时候，reduce就会超负荷
select
mid,money
from t_sale
order by money desc

--设置最大的reduce启动个数
set hive.exec.reducers.max=10;
--设置reduce的启动个数
set mapreduce.job.reduce=3

--hive数据类型
--数字类型
tinyint 1byte -128到127
smallint 2byte -32768到32767 char
int
bigint
float
double

--日期类型
timestamp
date

--字符串类型
string
varchar
char

--混杂类型
boolean
binary 二进制

--复合类型
--数组

流浪地球,吴京:吴孟达:张飞:赵云,2019-09-11
我和我的祖国,葛优:黄渤:宋佳:吴京,2019-10-01

create table t_movie(movie_name string,actors array<string>,show_time date)
row format delimited fields terminated by ','
collection items terminated by ':';

array_contains

select

from t_movie
where array_contains(actors,'张飞')

size

select
movie_name,actors,show_time,size(actors) num
from t_movie

========================================

map类型
1,zhangsan,father:xiaoming#mother:chentianxing#brother:shaoshuai,28
2,xiaoming,father:xiaoyun#mother:xiaohong#sister:xiaoyue#wife:chentianxing,30

create table t_user(id int,name string,family map<string,string>,jage int)
row format delimited fields terminated by ','
collection items terminated by '#'
map keys terminated by ':';

map_keys
select
id,name,map_keys(family) as relation,jage
from t_user;

map_values
select
id,name,map_values(family) as relation,jage
from t_user;

--家庭成员数量
select
id,name,size(family),jage
from t_user

--有兄弟的人
select

from t_user
where array_contains(map_keys(family),'brother')

========================================
1,zhangsan,18:man:beijing
2,lisi,22:woman:shanghai

create table t_people(id int,name string,info struct<age:int,sex:string,addr:string>)
row format delimited fields terminated by ','
collection items terminated by ':';

select
id,name,info.age
from t_people;

========================================

192,168,33,66,zhangsan,male
192,168,33,77,lisi,male
192,168,43,101,wangwu,female

create table t_people_ip(ip_seg1 string,ip_seg2 string,ip_seg3 string,ip_seg4 string,name string,sex string)
row format delimited fields terminated by ',';

--字符串拼接函数
concat
select concat('abc','def')

concat_ws
select concat_ws('.',ip_seg1,ip_seg2,ip_seg3,ip_seg4),name,sex from t_people_ip

===================================================================

--求字符串长度
length
select lenth('jsdfijsdkfjkdsfjkdf');

========================================

select to_date('2019-09-11 16:55:11');

--把字符串转换成unix时间戳
select unix_timestamp('2019-09-11 11:55:11','yyyy-MM-dd HH:mm:ss');

--把unix时间戳转换成字符串
select from_unixtime(unix_timestamp(),'yyyy-MM-dd HH:mm:ss');

========================================

--数学运算函数
--四舍五入
select round(5.4)
--四舍五入保留2位小数
select round(5.1345,2)
--向上取整
select ceil(5.3)
--向下取整
select floor(5.3)
--取绝对值
select abs(-5.2)
--取最大值
select greatest(3,4,5,6,7)
--取最小值
select least(3,4,5,6,7)

==================================================

1,19,a,male
2,19,b,male
3,22,c,female
4,16,d,female
5,30,e,male
6,26,f,female

+---------------+----------------+-----------------+----------------+--+
| t_student.id | t_student.age | t_student.name | t_student.sex |
+---------------+----------------+-----------------+----------------+--+
| 1 | 19 | a | male |
| 2 | 19 | b | male |
| 5 | 30 | e | male |

| 3 | 22 | c | female |
| 4 | 16 | d | female |
| 6 | 26 | f | female |

+---------------+----------------+-----------------+----------------+--+

row_number() over()

select
id,age,name,sex,row_number() over(partition by sex order by age desc) rk
from t_student

select
tmp.*
from
(select
id,age,name,sex,row_number() over(partition by sex order by age desc) rk
from t_student) tmp
where tmp.rk = 1

select
id,age,name,sex,rank() over(partition by sex order by age desc) rk
from t_student

select
id,age,name,sex,dense_rank() over(partition by sex order by age desc) rk
from t_student

========================================

A,2015-01,5
A,2015-01,15
B,2015-01,5
A,2015-01,8
B,2015-01,25
A,2015-01,5
C,2015-01,10
C,2015-01,20
A,2015-02,4
A,2015-02,6
C,2015-02,30
C,2015-02,10
B,2015-02,10
B,2015-02,5
A,2015-03,14
A,2015-03,6
B,2015-03,20
B,2015-03,25
C,2015-03,10
C,2015-03,20

--求每个用户每个月的销售额和到当月位置的累计销售额
create table t_saller(name string,month string,amount int)
row format delimited fields terminated by ','

create table t_accumulate
as
select name,month,sum(amount) samount
from t_saller
group by name,month

+--------------------+---------------------+-----------------------+--+
| t_accumulate.name | t_accumulate.month | t_accumulate.samount |
+--------------------+---------------------+-----------------------+--+
| A | 2015-01 | 33 |
| A | 2015-02 | 10 |
| A | 2015-03 | 20 |
| B | 2015-01 | 30 |
| B | 2015-02 | 15 |
| B | 2015-03 | 45 |
| C | 2015-01 | 30 |
| C | 2015-02 | 40 |
| C | 2015-03 | 30 |
+--------------------+---------------------+-----------------------+--+
--最前面一行
select
name,month,samount,sum(samount) over(partition by name order by month rows between unbounded preceding and current row) accumlateAmount
from
t_accumulate

| A | 2015-01 | 33 |
| A | 2015-02 | 10 |
| A | 2015-03 | 20 |
| A | 2015-04 | 30 |
| A | 2015-05 | 15 |
| A | 2015-06 | 45 |
| A | 2015-07 | 30 |
| A | 2015-08 | 40 |
| A | 2015-09 | 30 |

select
name,month,samount,sum(samount) over(partition by name order by month rows between 2 preceding and 1 following ) accumlateAmount
from
t_accumulate

preceding |
当前行 | 窗口长度
following |

min() over() ,max() over() , avg() over()

========================================

1,zhangsan,化学:物理:数学:语文
2,lisi,化学:数学:生物:生理:卫生
3,wangwu,化学:语文:英语:体育:生物

create table t_stu_subject(id int,name string,subjects array<string>)
row format delimited fields terminated by ','
collection items terminated by ':'

explode()

select explode(subjects) from t_stu_subject;

select distinct tmp.subs from (select explode(subjects) subs from t_stu_subject) tmp

========================================

lateral view连接函数

select
id,name,sub
from
t_stu_subject lateral view explode(subjects) tmp as sub

+-----+-----------+------+--+
| id | name | sub |
+-----+-----------+------+--+
| 1 | zhangsan | 化学 |
| 1 | zhangsan | 物理 |
| 1 | zhangsan | 数学 |
| 1 | zhangsan | 语文 |
| 2 | lisi | 化学 |
| 2 | lisi | 数学 |
| 2 | lisi | 生物 |
| 2 | lisi | 生理 |
| 2 | lisi | 卫生 |
| 3 | wangwu | 化学 |
| 3 | wangwu | 语文 |
| 3 | wangwu | 英语 |
| 3 | wangwu | 体育 |
| 3 | wangwu | 生物 |
+-----+-----------+------+--+

========================================

wordcount

words

create table words(line string)

line
hello world hi tom and jack
hello chentianxing qiaoyuan and shaoshuai
hello hello hi tom and shaoshuai
chentianxing love saoshuai
hello love what is love how love

split()

select
tmp.word,count(1) cnts
from
(select
explode(split(line,' ')) word
from words) tmp
group by tmp.word order by cnts desc

========================================

--炸map
select
id,name,key,value
from
t_user
lateral view explode(family) tmp as key,value

========================================

--有web系统，每天产生下列的日志文件

2017-09-15号的数据：
192.168.33.6,hunter,2017-09-15 10:30:20,/a
192.168.33.7,hunter,2017-09-15 10:30:26,/b
192.168.33.6,jack,2017-09-15 10:30:27,/a
192.168.33.8,tom,2017-09-15 10:30:28,/b
192.168.33.9,rose,2017-09-15 10:30:30,/b
192.168.33.10,julia,2017-09-15 10:30:40,/c

2017-09-16号的数据：
192.168.33.16,hunter,2017-09-16 10:30:20,/a
192.168.33.18,jerry,2017-09-16 10:30:30,/b
192.168.33.26,jack,2017-09-16 10:30:40,/a
192.168.33.18,polo,2017-09-16 10:30:50,/b
192.168.33.39,nissan,2017-09-16 10:30:53,/b
192.168.33.39,nissan,2017-09-16 10:30:55,/a
192.168.33.39,nissan,2017-09-16 10:30:58,/c
192.168.33.20,ford,2017-09-16 10:30:54,/c

2017-09-17号的数据：
192.168.33.46,hunter,2017-09-17 10:30:21,/a
192.168.43.18,jerry,2017-09-17 10:30:22,/b
192.168.43.26,tom,2017-09-17 10:30:23,/a
192.168.53.18,bmw,2017-09-17 10:30:24,/b
192.168.63.39,benz,2017-09-17 10:30:25,/b
192.168.33.25,haval,2017-09-17 10:30:30,/c
192.168.33.10,julia,2017-09-17 10:30:40,/c

--统计日活跃用户

--统计每日新增用户

--统计历史用户

--创建日志表
create table t_web_log(ip string,uname string,access_time string,url string)
partitioned by (day string)
row format delimited fields terminated by ',';

--创建日活跃用户表
create table t_user_active_day
like
t_web_log;

--统计日活跃用户
insert into table t_user_active_day partition(day='2017-09-15')
select tmp.ip,tmp.uname,tmp.access_time,tmp.url
from
(select
ip,uname,access_time,url,row_number() over(partition by uname order by access_time) rn
from
t_web_log where day='2017-09-15') tmp
where rn <2

--创建历史用户表
create table t_user_history(uname string)

--创建日新增用户表
create table t_user_new_day like t_user_active_day;

--统计日新增用户
insert into table t_user_new_day partition(day='2017-09-15')
select
tua.ip,tua.uname,tua.access_time,tua.url
from t_user_active_day tua
left join t_user_history tuh on tua.uname = tuh.uname
where tua.day = '2017-09-15' and tuh.uname IS NULL

--记录历史用户
insert into table t_user_history
select
uname
from t_user_new_day
where day='2017-09-15'

insert into table t_user_active_day partition(day='2017-09-16')
select tmp.ip,tmp.uname,tmp.access_time,tmp.url
from
(select
ip,uname,access_time,url,row_number() over(partition by uname order by access_time) rn
from
t_web_log where day='2017-09-16') tmp
where rn <2

insert into table t_user_new_day partition(day='2017-09-16')
select
tua.ip,tua.uname,tua.access_time,tua.url
from t_user_active_day tua
left join t_user_history tuh on tua.uname = tuh.uname
where tua.day = '2017-09-16' and tuh.uname IS NULL

insert into table t_user_history
select
uname
from t_user_new_day
where day='2017-09-16'

--桶表
--创建桶表，按id分成三个桶
create table t1(id int,name string,age int)
clustered by(id) into 3 buckets
row format delimited fields terminated by ',';

--桶表插入数据的方式
create table t1_1(id int,name string,age int)
row format delimited fields terminated by ',';

set hive.enforce.bucketing=true;
set mapreduce.job.reduces=3;

insert into t1 select * from t1_1;

========================================
hive自定义函数
package com.bwie.hive.udf;

import com.alibaba.fastjson.JSONObject;
import org.apache.hadoop.hive.ql.exec.UDF;

public class JsonUDF extends UDF {

public String evaluate(String json,String colum){
    JSONObject jsonObject = JSONObject.parseObject(json);
    String value = jsonObject.getString(colum);
    return value;
}
}

打包上传
add jar /root/hivetest-1.0-SNAPSHOT.jar;
create temporary function getjson as 'com.bwie.hive.udf.JsonUDF';

select
getjson(json,'movie') as movie,getjson(json,'rate') as rate,from_unixtime(CAST(getjson(json,'timeStamp') as BIGINT),'yyyy-MM-dd HH:mm:ss') as showtime,getjson(json,'uid') as uid
from t_ex_rat limit 10;

--创建永久函数
方法一、
在hive-site.xml里添加jar包
<property>
<name>hive.aux.jars.path</name>
<value>file:///usr/local/apache-hive-1.2.2-bin/lib/hivetest-1.0-SNAPSHOT.jar</value>
</property>
疑似jar包要放到lib下才能生效
create function getjson AS 'com.bwie.hive.udf.JsonUDF'

方法二、
create function getjson as 'com.bwie.hive.udf.JsonUDF' using jar 'hdfs://hdp1703/lib/hivetest-1.0-SNAPSHOT.jar'

drop function getjson

========================================

create table t_employee(id int,name string,age int)
partitioned by(state string,city string)
row format delimited fields terminated by ','

create table t_employee_orgin(id int,name string,age int,state string,city string)
row format delimited fields terminated by ',';

load data local inpath '/root/employ.txt' into table t_employee_orgin;

在严格模式下，多个分区一定有一个分区是固定的
insert into t_employee
partition(state='china',city)
select id,name,age,city
from t_employee_orgin
where state='china'

--非严格模式
set hive.exec.dynamic.partition.mode=nostrict;
insert into t_employee
partition(state,city)
select id,name,age,state,city
from t_employee_orgin

========================================

select to_date('2019-09-20 17:30:00')

unix_timestamp('2019/09/20 17:30:00','yyyy/MM/dd HH:mm:ss')

select from_unixtime(unix_timestamp('2019/09/20 17:30:00','yyyy/MM/dd HH:mm:ss'),'yyyy-MM-dd HH:mm:ss')。">
<meta property="og:title" content="hive语句大全">
<meta property="og:description" content="hive数据库是hdfs上的文件夹，表也是文件夹，表里的数据是文件
hive建表
create table t_student(id string,name string,age int,classNo string)
row format delimited
fields terminated by ',';

创建外部表
create external table t_a(id string,name string)
row format delimited
fields terminated by ','
location '/.../...'
外部表和内部表的区别，drop时内部表的hdfs数据一同删除，外部表的hdfs上的数据不删除

create table t_b(id string,name string)
row format delimited
fields terminated by ','

create table t_a(id string,name string)
row format delimited
fields terminated by ','

--加载数据从hdfs上加载
load data inpath '/dataB/b.txt' into table t_a;
--加载数据从本地上加载
load data local inpath '/root/bb.txt' into table t_b;

t_a
+---------+-----------+--+
| t_a.id | t_a.name |
+---------+-----------+--+
| a | 1 |
| b | 2 |
| c | 4 |
| d | 5 |
+---------+-----------+--+

t_b
+---------+-----------+--+
| t_b.id | t_b.name |
+---------+-----------+--+
| a | xx |
| b | yy |
| d | zz |
| e | pp |
+---------+-----------+--+

--笛卡尔积
select a.,b.
from t_a inner join t_b

+---------+-----------+---------+-----------+--+
| t_a.id | t_a.name | t_b.id | t_b.name |
+---------+-----------+---------+-----------+--+
| a | 1 | a | xx |
| b | 2 | a | xx |
| c | 4 | a | xx |
| d | 5 | a | xx |
| a | 1 | b | yy |
| b | 2 | b | yy |
| c | 4 | b | yy |
| d | 5 | b | yy |
| a | 1 | d | zz |
| b | 2 | d | zz |
| c | 4 | d | zz |
| d | 5 | d | zz |
| a | 1 | e | pp |
| b | 2 | e | pp |
| c | 4 | e | pp |
| d | 5 | e | pp |
+---------+-----------+---------+-----------+--+

--内连接
select * from t_a join t_b where t_a.id = t_b.id;
select * from t_a join t_b on t_a.id = t_b.id;

--左外连接
select
a.,b.
from t_a a left join t_b b

select
a.,b.
from t_a a left join t_b b on a.id = b.id

+-------+---------+-------+---------+--+
| a.id | a.name | b.id | b.name |
+-------+---------+-------+---------+--+
| a | 1 | a | xx |
| b | 2 | b | yy |
| d | 5 | d | zz |
| NULL | NULL | e | pp |
+-------+---------+-------+---------+--+

--右外连接
select
a.,b.
from t_a a right outer join t_b b

select
a.,b.
from t_a a right outer join t_b b on a.id = b.id
+-------+---------+-------+---------+--+
| a.id | a.name | b.id | b.name |
+-------+---------+-------+---------+--+
| a | 1 | a | xx |
| b | 2 | b | yy |
| d | 5 | d | zz |
| NULL | NULL | e | pp |
+-------+---------+-------+---------+--+

--全外连接
select
a.,b.
from t_a a full outer join t_b b on a.id = b.id

+-------+---------+-------+---------+--+
| a.id | a.name | b.id | b.name |
+-------+---------+-------+---------+--+
| a | 1 | a | xx |
| b | 2 | b | yy |
| c | 4 | NULL | NULL |
| d | 5 | d | zz |
| NULL | NULL | e | pp |
+-------+---------+-------+---------+--+

--左半连接 相当于指定条件的内连接，但是只显示左边的表数据
select
a.*
from t_a a left semi join t_b b on a.id = b.id

+-------+---------+--+
| a.id | a.name |
+-------+---------+--+
| a | 1 |
| b | 2 |
| d | 5 |
+-------+---------+--+

--例如有这些数据
vi access.log.0804
192.168.33.3,http://www.sina.com/stu,2017-08-04 15:30:20
192.168.33.3,http://www.sina.com/teach,2017-08-04 15:35:20
192.168.33.4,http://www.sina.com/stu,2017-08-04 15:30:20
192.168.33.4,http://www.sina.com/job,2017-08-04 16:30:20
192.168.33.5,http://www.sina.com/job,2017-08-04 15:40:20

vi access.log.0805
192.168.33.3,http://www.sina.com/stu,2017-08-05 15:30:20
192.168.44.3,http://www.sina.com/teach,2017-08-05 15:35:20
192.168.33.44,http://www.sina.com/stu,2017-08-05 15:30:20
192.168.33.46,http://www.sina.com/job,2017-08-05 16:30:20
192.168.33.55,http://www.sina.com/job,2017-08-05 15:40:20

vi access.log.0806
192.168.133.3,http://www.sina.com/register,2017-08-06 15:30:20
192.168.111.3,http://www.sina.com/register,2017-08-06 15:35:20
192.168.34.44,http://www.sina.com/pay,2017-08-06 15:30:20
192.168.33.46,http://www.sina.com/excersize,2017-08-06 16:30:20
192.168.33.55,http://www.sina.com/job,2017-08-06 15:40:20
192.168.33.46,http://www.sina.com/excersize,2017-08-06 16:30:20
192.168.33.25,http://www.sina.com/job,2017-08-06 15:40:20
192.168.33.36,http://www.sina.com/excersize,2017-08-06 16:30:20
192.168.33.55,http://www.sina.com/job,2017-08-06 15:40:20

--建分区表
create table t_access(ip string,url string,access_time string)
partitioned by (day string)
row format delimited
fields terminated by ','

--导入分区表数据
load data local inpath '/root/access.log.0806' into table t_access partition(day='2017-08-06');

--查看当前分区情况
show partitions t_access;

--求每条url访问的次数
select
url,count(1)
from t_access
group by url;

--求每条url访问的ip中，最大的ip是哪个
select
url,max(ip)
from t_access
group by url;

--求每个ip访问同一个页面的记录中，最晚的一条
select
ip,url,max(access_time)
from t_access
group by ip,url

--求PV
select
url,count(1)
from t_access
group by url

+--------------------------------+------+--+
| url | _c1 |
+--------------------------------+------+--+
| http://www.sina.com/excersize | 3 |
| http://www.sina.com/job | 7 |
| http://www.sina.com/pay | 1 |
| http://www.sina.com/register | 2 |
| http://www.sina.com/stu | 4 |
| http://www.sina.com/teach | 2 |
+--------------------------------+------+--+

--求UV
select
url,count(distinct(ip))
from t_access
group by url

+--------------------------------+-----+--+
| url | c1 |
+--------------------------------+-----+--+
| http://www.sina.com/excersize | 2 |
| http://www.sina.com/job | 5 |
| http://www.sina.com/pay | 1 |
| http://www.sina.com/register | 2 |
| http://www.sina.com/stu | 3 |
| http://www.sina.com/teach | 2 |
+--------------------------------+-----+--+

--求每条url访问的ip中，最大的ip是哪个
--PV
--UV
--用mapreduce实现

--求8月6号pv，uv
select
count(1),url
from t_access
where day = '2017-08-06'
group by url

--求8月4号以后，每天访问http://www.sina.com/job 的次数，以及访问者中ip最大的
select
count(1),max(ip),day
from t_access
where day > '2017-08-04'
and url = 'http://www.sina.com/job'
group by day

select
count(1),max(ip),day
from t_access
where url = 'http://www.sina.com/job'
group by day having day > '2017-08-04'

--求8月4号以后，每天每个页面访问总次数，以及页面最大的ip，并且上述查询结果中访问次数大于2的
select tmp.* from
(select
url,day,count(1) cnts,max(ip) ip
from t_access
group by day,url having day > '2017-08-04') tmp
where tmp.cnts > 2

--每天，pv排序
select
url,day,count(1) cnts
from t_access
group by day,url
order by cnts desc

--hive里的order by是全局排序
--sort by是在map里进行排序

select
tmp.*
from
(select
url,day,count(1) cnts
from t_access
group by day,url) tmp
distribute by tmp.day sort by tmp.cnts desc

--有如下数据
+-------------+---------------+--+
| t_sale.mid | t_sale.money |
+-------------+---------------+--+
| AA | 15.0 |
| AA | 20.0 |
| BB | 22.0 |
| CC | 44.0 |
+-------------+---------------+--+

--distribute by先把数据分发到各个reduce中，然后sort by在各个reduce中进行局部排序
select
mid,money
from t_sale
distribute by mid sort by money desc

+------+--------+--+
| mid | money |
+------+--------+--+
| AA | 15.0 |
| AA | 20.0 |
| BB | 22.0 |
| CC | 44.0 |
+------+--------+--+

--cluster by mid 等于 distribute by mid sort by mid
--cluster by后面不能跟desc，asc，默认的只能升序

--order by 是全排序，所有的数据会发送给一个reduceTask进行处理，在数据量大的时候，reduce就会超负荷
select
mid,money
from t_sale
order by money desc

--设置最大的reduce启动个数
set hive.exec.reducers.max=10;
--设置reduce的启动个数
set mapreduce.job.reduce=3

--hive数据类型
--数字类型
tinyint 1byte -128到127
smallint 2byte -32768到32767 char
int
bigint
float
double

--日期类型
timestamp
date

--字符串类型
string
varchar
char

--混杂类型
boolean
binary 二进制

--复合类型
--数组

流浪地球,吴京:吴孟达:张飞:赵云,2019-09-11
我和我的祖国,葛优:黄渤:宋佳:吴京,2019-10-01

create table t_movie(movie_name string,actors array<string>,show_time date)
row format delimited fields terminated by ','
collection items terminated by ':';

array_contains

select

from t_movie
where array_contains(actors,'张飞')

size

select
movie_name,actors,show_time,size(actors) num
from t_movie

========================================

map类型
1,zhangsan,father:xiaoming#mother:chentianxing#brother:shaoshuai,28
2,xiaoming,father:xiaoyun#mother:xiaohong#sister:xiaoyue#wife:chentianxing,30

create table t_user(id int,name string,family map<string,string>,jage int)
row format delimited fields terminated by ','
collection items terminated by '#'
map keys terminated by ':';

map_keys
select
id,name,map_keys(family) as relation,jage
from t_user;

map_values
select
id,name,map_values(family) as relation,jage
from t_user;

--家庭成员数量
select
id,name,size(family),jage
from t_user

--有兄弟的人
select

from t_user
where array_contains(map_keys(family),'brother')

========================================
1,zhangsan,18:man:beijing
2,lisi,22:woman:shanghai

create table t_people(id int,name string,info struct<age:int,sex:string,addr:string>)
row format delimited fields terminated by ','
collection items terminated by ':';

select
id,name,info.age
from t_people;

========================================

192,168,33,66,zhangsan,male
192,168,33,77,lisi,male
192,168,43,101,wangwu,female

create table t_people_ip(ip_seg1 string,ip_seg2 string,ip_seg3 string,ip_seg4 string,name string,sex string)
row format delimited fields terminated by ',';

--字符串拼接函数
concat
select concat('abc','def')

concat_ws
select concat_ws('.',ip_seg1,ip_seg2,ip_seg3,ip_seg4),name,sex from t_people_ip

===================================================================

--求字符串长度
length
select lenth('jsdfijsdkfjkdsfjkdf');

========================================

select to_date('2019-09-11 16:55:11');

--把字符串转换成unix时间戳
select unix_timestamp('2019-09-11 11:55:11','yyyy-MM-dd HH:mm:ss');

--把unix时间戳转换成字符串
select from_unixtime(unix_timestamp(),'yyyy-MM-dd HH:mm:ss');

========================================

--数学运算函数
--四舍五入
select round(5.4)
--四舍五入保留2位小数
select round(5.1345,2)
--向上取整
select ceil(5.3)
--向下取整
select floor(5.3)
--取绝对值
select abs(-5.2)
--取最大值
select greatest(3,4,5,6,7)
--取最小值
select least(3,4,5,6,7)

==================================================

1,19,a,male
2,19,b,male
3,22,c,female
4,16,d,female
5,30,e,male
6,26,f,female

+---------------+----------------+-----------------+----------------+--+
| t_student.id | t_student.age | t_student.name | t_student.sex |
+---------------+----------------+-----------------+----------------+--+
| 1 | 19 | a | male |
| 2 | 19 | b | male |
| 5 | 30 | e | male |

| 3 | 22 | c | female |
| 4 | 16 | d | female |
| 6 | 26 | f | female |

+---------------+----------------+-----------------+----------------+--+

row_number() over()

select
id,age,name,sex,row_number() over(partition by sex order by age desc) rk
from t_student

select
tmp.*
from
(select
id,age,name,sex,row_number() over(partition by sex order by age desc) rk
from t_student) tmp
where tmp.rk = 1

select
id,age,name,sex,rank() over(partition by sex order by age desc) rk
from t_student

select
id,age,name,sex,dense_rank() over(partition by sex order by age desc) rk
from t_student

========================================

A,2015-01,5
A,2015-01,15
B,2015-01,5
A,2015-01,8
B,2015-01,25
A,2015-01,5
C,2015-01,10
C,2015-01,20
A,2015-02,4
A,2015-02,6
C,2015-02,30
C,2015-02,10
B,2015-02,10
B,2015-02,5
A,2015-03,14
A,2015-03,6
B,2015-03,20
B,2015-03,25
C,2015-03,10
C,2015-03,20

--求每个用户每个月的销售额和到当月位置的累计销售额
create table t_saller(name string,month string,amount int)
row format delimited fields terminated by ','

create table t_accumulate
as
select name,month,sum(amount) samount
from t_saller
group by name,month

+--------------------+---------------------+-----------------------+--+
| t_accumulate.name | t_accumulate.month | t_accumulate.samount |
+--------------------+---------------------+-----------------------+--+
| A | 2015-01 | 33 |
| A | 2015-02 | 10 |
| A | 2015-03 | 20 |
| B | 2015-01 | 30 |
| B | 2015-02 | 15 |
| B | 2015-03 | 45 |
| C | 2015-01 | 30 |
| C | 2015-02 | 40 |
| C | 2015-03 | 30 |
+--------------------+---------------------+-----------------------+--+
--最前面一行
select
name,month,samount,sum(samount) over(partition by name order by month rows between unbounded preceding and current row) accumlateAmount
from
t_accumulate

| A | 2015-01 | 33 |
| A | 2015-02 | 10 |
| A | 2015-03 | 20 |
| A | 2015-04 | 30 |
| A | 2015-05 | 15 |
| A | 2015-06 | 45 |
| A | 2015-07 | 30 |
| A | 2015-08 | 40 |
| A | 2015-09 | 30 |

select
name,month,samount,sum(samount) over(partition by name order by month rows between 2 preceding and 1 following ) accumlateAmount
from
t_accumulate

preceding |
当前行 | 窗口长度
following |

min() over() ,max() over() , avg() over()

========================================

1,zhangsan,化学:物理:数学:语文
2,lisi,化学:数学:生物:生理:卫生
3,wangwu,化学:语文:英语:体育:生物

create table t_stu_subject(id int,name string,subjects array<string>)
row format delimited fields terminated by ','
collection items terminated by ':'

explode()

select explode(subjects) from t_stu_subject;

select distinct tmp.subs from (select explode(subjects) subs from t_stu_subject) tmp

========================================

lateral view连接函数

select
id,name,sub
from
t_stu_subject lateral view explode(subjects) tmp as sub

+-----+-----------+------+--+
| id | name | sub |
+-----+-----------+------+--+
| 1 | zhangsan | 化学 |
| 1 | zhangsan | 物理 |
| 1 | zhangsan | 数学 |
| 1 | zhangsan | 语文 |
| 2 | lisi | 化学 |
| 2 | lisi | 数学 |
| 2 | lisi | 生物 |
| 2 | lisi | 生理 |
| 2 | lisi | 卫生 |
| 3 | wangwu | 化学 |
| 3 | wangwu | 语文 |
| 3 | wangwu | 英语 |
| 3 | wangwu | 体育 |
| 3 | wangwu | 生物 |
+-----+-----------+------+--+

========================================

wordcount

words

create table words(line string)

line
hello world hi tom and jack
hello chentianxing qiaoyuan and shaoshuai
hello hello hi tom and shaoshuai
chentianxing love saoshuai
hello love what is love how love

split()

select
tmp.word,count(1) cnts
from
(select
explode(split(line,' ')) word
from words) tmp
group by tmp.word order by cnts desc

========================================

--炸map
select
id,name,key,value
from
t_user
lateral view explode(family) tmp as key,value

========================================

--有web系统，每天产生下列的日志文件

2017-09-15号的数据：
192.168.33.6,hunter,2017-09-15 10:30:20,/a
192.168.33.7,hunter,2017-09-15 10:30:26,/b
192.168.33.6,jack,2017-09-15 10:30:27,/a
192.168.33.8,tom,2017-09-15 10:30:28,/b
192.168.33.9,rose,2017-09-15 10:30:30,/b
192.168.33.10,julia,2017-09-15 10:30:40,/c

2017-09-16号的数据：
192.168.33.16,hunter,2017-09-16 10:30:20,/a
192.168.33.18,jerry,2017-09-16 10:30:30,/b
192.168.33.26,jack,2017-09-16 10:30:40,/a
192.168.33.18,polo,2017-09-16 10:30:50,/b
192.168.33.39,nissan,2017-09-16 10:30:53,/b
192.168.33.39,nissan,2017-09-16 10:30:55,/a
192.168.33.39,nissan,2017-09-16 10:30:58,/c
192.168.33.20,ford,2017-09-16 10:30:54,/c

2017-09-17号的数据：
192.168.33.46,hunter,2017-09-17 10:30:21,/a
192.168.43.18,jerry,2017-09-17 10:30:22,/b
192.168.43.26,tom,2017-09-17 10:30:23,/a
192.168.53.18,bmw,2017-09-17 10:30:24,/b
192.168.63.39,benz,2017-09-17 10:30:25,/b
192.168.33.25,haval,2017-09-17 10:30:30,/c
192.168.33.10,julia,2017-09-17 10:30:40,/c

--统计日活跃用户

--统计每日新增用户

--统计历史用户

--创建日志表
create table t_web_log(ip string,uname string,access_time string,url string)
partitioned by (day string)
row format delimited fields terminated by ',';

--创建日活跃用户表
create table t_user_active_day
like
t_web_log;

--统计日活跃用户
insert into table t_user_active_day partition(day='2017-09-15')
select tmp.ip,tmp.uname,tmp.access_time,tmp.url
from
(select
ip,uname,access_time,url,row_number() over(partition by uname order by access_time) rn
from
t_web_log where day='2017-09-15') tmp
where rn <2

--创建历史用户表
create table t_user_history(uname string)

--创建日新增用户表
create table t_user_new_day like t_user_active_day;

--统计日新增用户
insert into table t_user_new_day partition(day='2017-09-15')
select
tua.ip,tua.uname,tua.access_time,tua.url
from t_user_active_day tua
left join t_user_history tuh on tua.uname = tuh.uname
where tua.day = '2017-09-15' and tuh.uname IS NULL

--记录历史用户
insert into table t_user_history
select
uname
from t_user_new_day
where day='2017-09-15'

insert into table t_user_active_day partition(day='2017-09-16')
select tmp.ip,tmp.uname,tmp.access_time,tmp.url
from
(select
ip,uname,access_time,url,row_number() over(partition by uname order by access_time) rn
from
t_web_log where day='2017-09-16') tmp
where rn <2

insert into table t_user_new_day partition(day='2017-09-16')
select
tua.ip,tua.uname,tua.access_time,tua.url
from t_user_active_day tua
left join t_user_history tuh on tua.uname = tuh.uname
where tua.day = '2017-09-16' and tuh.uname IS NULL

insert into table t_user_history
select
uname
from t_user_new_day
where day='2017-09-16'

--桶表
--创建桶表，按id分成三个桶
create table t1(id int,name string,age int)
clustered by(id) into 3 buckets
row format delimited fields terminated by ',';

--桶表插入数据的方式
create table t1_1(id int,name string,age int)
row format delimited fields terminated by ',';

set hive.enforce.bucketing=true;
set mapreduce.job.reduces=3;

insert into t1 select * from t1_1;

========================================
hive自定义函数
package com.bwie.hive.udf;

import com.alibaba.fastjson.JSONObject;
import org.apache.hadoop.hive.ql.exec.UDF;

public class JsonUDF extends UDF {

public String evaluate(String json,String colum){
    JSONObject jsonObject = JSONObject.parseObject(json);
    String value = jsonObject.getString(colum);
    return value;
}
}

打包上传
add jar /root/hivetest-1.0-SNAPSHOT.jar;
create temporary function getjson as 'com.bwie.hive.udf.JsonUDF';

select
getjson(json,'movie') as movie,getjson(json,'rate') as rate,from_unixtime(CAST(getjson(json,'timeStamp') as BIGINT),'yyyy-MM-dd HH:mm:ss') as showtime,getjson(json,'uid') as uid
from t_ex_rat limit 10;

--创建永久函数
方法一、
在hive-site.xml里添加jar包
<property>
<name>hive.aux.jars.path</name>
<value>file:///usr/local/apache-hive-1.2.2-bin/lib/hivetest-1.0-SNAPSHOT.jar</value>
</property>
疑似jar包要放到lib下才能生效
create function getjson AS 'com.bwie.hive.udf.JsonUDF'

方法二、
create function getjson as 'com.bwie.hive.udf.JsonUDF' using jar 'hdfs://hdp1703/lib/hivetest-1.0-SNAPSHOT.jar'

drop function getjson

========================================

create table t_employee(id int,name string,age int)
partitioned by(state string,city string)
row format delimited fields terminated by ','

create table t_employee_orgin(id int,name string,age int,state string,city string)
row format delimited fields terminated by ',';

load data local inpath '/root/employ.txt' into table t_employee_orgin;

在严格模式下，多个分区一定有一个分区是固定的
insert into t_employee
partition(state='china',city)
select id,name,age,city
from t_employee_orgin
where state='china'

--非严格模式
set hive.exec.dynamic.partition.mode=nostrict;
insert into t_employee
partition(state,city)
select id,name,age,state,city
from t_employee_orgin

========================================

select to_date('2019-09-20 17:30:00')

unix_timestamp('2019/09/20 17:30:00','yyyy/MM/dd HH:mm:ss')

select from_unixtime(unix_timestamp('2019/09/20 17:30:00','yyyy/MM/dd HH:mm:ss'),'yyyy-MM-dd HH:mm:ss')。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://0or1world.github.io/post/hive-yu-ju-da-quan.html">
<meta property="og:image" content="https://github.githubassets.com/favicons/favicon.svg">
<title>hive语句大全</title>


</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}
</style>




<body>
    <div id="header">
<h1 class="postTitle">hive语句大全</h1>
<div class="title-right">
    <a href="https://0or1world.github.io" id="buttonHome" class="btn btn-invisible circle" title="首页">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/0or1world/0or1world.github.io/issues/1" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="切换主题">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><p>hive数据库是hdfs上的文件夹，表也是文件夹，表里的数据是文件<br>
hive建表<br>
create table t_student(id string,name string,age int,classNo string)<br>
row format delimited<br>
fields terminated by ',';</p>
<p>创建外部表<br>
create external table t_a(id string,name string)<br>
row format delimited<br>
fields terminated by ','<br>
location '/.../...'<br>
外部表和内部表的区别，drop时内部表的hdfs数据一同删除，外部表的hdfs上的数据不删除</p>
<p>create table t_b(id string,name string)<br>
row format delimited<br>
fields terminated by ','</p>
<p>create table t_a(id string,name string)<br>
row format delimited<br>
fields terminated by ','</p>
<p>--加载数据从hdfs上加载<br>
load data inpath '/dataB/b.txt' into table t_a;<br>
--加载数据从本地上加载<br>
load data local inpath '/root/bb.txt' into table t_b;</p>
<p>t_a<br>
+---------+-----------+--+<br>
| t_a.id | t_a.name |<br>
+---------+-----------+--+<br>
| a | 1 |<br>
| b | 2 |<br>
| c | 4 |<br>
| d | 5 |<br>
+---------+-----------+--+</p>
<p>t_b<br>
+---------+-----------+--+<br>
| t_b.id | t_b.name |<br>
+---------+-----------+--+<br>
| a | xx |<br>
| b | yy |<br>
| d | zz |<br>
| e | pp |<br>
+---------+-----------+--+</p>
<p>--笛卡尔积<br>
select a.,b.<br>
from t_a inner join t_b</p>
<p>+---------+-----------+---------+-----------+--+<br>
| t_a.id | t_a.name | t_b.id | t_b.name |<br>
+---------+-----------+---------+-----------+--+<br>
| a | 1 | a | xx |<br>
| b | 2 | a | xx |<br>
| c | 4 | a | xx |<br>
| d | 5 | a | xx |<br>
| a | 1 | b | yy |<br>
| b | 2 | b | yy |<br>
| c | 4 | b | yy |<br>
| d | 5 | b | yy |<br>
| a | 1 | d | zz |<br>
| b | 2 | d | zz |<br>
| c | 4 | d | zz |<br>
| d | 5 | d | zz |<br>
| a | 1 | e | pp |<br>
| b | 2 | e | pp |<br>
| c | 4 | e | pp |<br>
| d | 5 | e | pp |<br>
+---------+-----------+---------+-----------+--+</p>
<p>--内连接<br>
select * from t_a join t_b where t_a.id = t_b.id;<br>
select * from t_a join t_b on t_a.id = t_b.id;</p>
<p>--左外连接<br>
select<br>
a.,b.<br>
from t_a a left join t_b b</p>
<p>select<br>
a.,b.<br>
from t_a a left join t_b b on a.id = b.id</p>
<p>+-------+---------+-------+---------+--+<br>
| a.id | a.name | b.id | b.name |<br>
+-------+---------+-------+---------+--+<br>
| a | 1 | a | xx |<br>
| b | 2 | b | yy |<br>
| d | 5 | d | zz |<br>
| NULL | NULL | e | pp |<br>
+-------+---------+-------+---------+--+</p>
<p>--右外连接<br>
select<br>
a.,b.<br>
from t_a a right outer join t_b b</p>
<p>select<br>
a.,b.<br>
from t_a a right outer join t_b b on a.id = b.id<br>
+-------+---------+-------+---------+--+<br>
| a.id | a.name | b.id | b.name |<br>
+-------+---------+-------+---------+--+<br>
| a | 1 | a | xx |<br>
| b | 2 | b | yy |<br>
| d | 5 | d | zz |<br>
| NULL | NULL | e | pp |<br>
+-------+---------+-------+---------+--+</p>
<p>--全外连接<br>
select<br>
a.,b.<br>
from t_a a full outer join t_b b on a.id = b.id</p>
<p>+-------+---------+-------+---------+--+<br>
| a.id | a.name | b.id | b.name |<br>
+-------+---------+-------+---------+--+<br>
| a | 1 | a | xx |<br>
| b | 2 | b | yy |<br>
| c | 4 | NULL | NULL |<br>
| d | 5 | d | zz |<br>
| NULL | NULL | e | pp |<br>
+-------+---------+-------+---------+--+</p>
<p>--左半连接 相当于指定条件的内连接，但是只显示左边的表数据<br>
select<br>
a.*<br>
from t_a a left semi join t_b b on a.id = b.id</p>
<p>+-------+---------+--+<br>
| a.id | a.name |<br>
+-------+---------+--+<br>
| a | 1 |<br>
| b | 2 |<br>
| d | 5 |<br>
+-------+---------+--+</p>
<p>--例如有这些数据<br>
vi access.log.0804<br>
192.168.33.3,<a href="http://www.sina.com/stu,2017-08-04" rel="nofollow">http://www.sina.com/stu,2017-08-04</a> 15:30:20<br>
192.168.33.3,<a href="http://www.sina.com/teach,2017-08-04" rel="nofollow">http://www.sina.com/teach,2017-08-04</a> 15:35:20<br>
192.168.33.4,<a href="http://www.sina.com/stu,2017-08-04" rel="nofollow">http://www.sina.com/stu,2017-08-04</a> 15:30:20<br>
192.168.33.4,<a href="http://www.sina.com/job,2017-08-04" rel="nofollow">http://www.sina.com/job,2017-08-04</a> 16:30:20<br>
192.168.33.5,<a href="http://www.sina.com/job,2017-08-04" rel="nofollow">http://www.sina.com/job,2017-08-04</a> 15:40:20</p>
<p>vi access.log.0805<br>
192.168.33.3,<a href="http://www.sina.com/stu,2017-08-05" rel="nofollow">http://www.sina.com/stu,2017-08-05</a> 15:30:20<br>
192.168.44.3,<a href="http://www.sina.com/teach,2017-08-05" rel="nofollow">http://www.sina.com/teach,2017-08-05</a> 15:35:20<br>
192.168.33.44,<a href="http://www.sina.com/stu,2017-08-05" rel="nofollow">http://www.sina.com/stu,2017-08-05</a> 15:30:20<br>
192.168.33.46,<a href="http://www.sina.com/job,2017-08-05" rel="nofollow">http://www.sina.com/job,2017-08-05</a> 16:30:20<br>
192.168.33.55,<a href="http://www.sina.com/job,2017-08-05" rel="nofollow">http://www.sina.com/job,2017-08-05</a> 15:40:20</p>
<p>vi access.log.0806<br>
192.168.133.3,<a href="http://www.sina.com/register,2017-08-06" rel="nofollow">http://www.sina.com/register,2017-08-06</a> 15:30:20<br>
192.168.111.3,<a href="http://www.sina.com/register,2017-08-06" rel="nofollow">http://www.sina.com/register,2017-08-06</a> 15:35:20<br>
192.168.34.44,<a href="http://www.sina.com/pay,2017-08-06" rel="nofollow">http://www.sina.com/pay,2017-08-06</a> 15:30:20<br>
192.168.33.46,<a href="http://www.sina.com/excersize,2017-08-06" rel="nofollow">http://www.sina.com/excersize,2017-08-06</a> 16:30:20<br>
192.168.33.55,<a href="http://www.sina.com/job,2017-08-06" rel="nofollow">http://www.sina.com/job,2017-08-06</a> 15:40:20<br>
192.168.33.46,<a href="http://www.sina.com/excersize,2017-08-06" rel="nofollow">http://www.sina.com/excersize,2017-08-06</a> 16:30:20<br>
192.168.33.25,<a href="http://www.sina.com/job,2017-08-06" rel="nofollow">http://www.sina.com/job,2017-08-06</a> 15:40:20<br>
192.168.33.36,<a href="http://www.sina.com/excersize,2017-08-06" rel="nofollow">http://www.sina.com/excersize,2017-08-06</a> 16:30:20<br>
192.168.33.55,<a href="http://www.sina.com/job,2017-08-06" rel="nofollow">http://www.sina.com/job,2017-08-06</a> 15:40:20</p>
<p>--建分区表<br>
create table t_access(ip string,url string,access_time string)<br>
partitioned by (day string)<br>
row format delimited<br>
fields terminated by ','</p>
<p>--导入分区表数据<br>
load data local inpath '/root/access.log.0806' into table t_access partition(day='2017-08-06');</p>
<p>--查看当前分区情况<br>
show partitions t_access;</p>
<p>--求每条url访问的次数<br>
select<br>
url,count(1)<br>
from t_access<br>
group by url;</p>
<p>--求每条url访问的ip中，最大的ip是哪个<br>
select<br>
url,max(ip)<br>
from t_access<br>
group by url;</p>
<p>--求每个ip访问同一个页面的记录中，最晚的一条<br>
select<br>
ip,url,max(access_time)<br>
from t_access<br>
group by ip,url</p>
<p>--求PV<br>
select<br>
url,count(1)<br>
from t_access<br>
group by url</p>
<p>+--------------------------------+------+--+<br>
| url | _c1 |<br>
+--------------------------------+------+--+<br>
| <a href="http://www.sina.com/excersize" rel="nofollow">http://www.sina.com/excersize</a> | 3 |<br>
| <a href="http://www.sina.com/job" rel="nofollow">http://www.sina.com/job</a> | 7 |<br>
| <a href="http://www.sina.com/pay" rel="nofollow">http://www.sina.com/pay</a> | 1 |<br>
| <a href="http://www.sina.com/register" rel="nofollow">http://www.sina.com/register</a> | 2 |<br>
| <a href="http://www.sina.com/stu" rel="nofollow">http://www.sina.com/stu</a> | 4 |<br>
| <a href="http://www.sina.com/teach" rel="nofollow">http://www.sina.com/teach</a> | 2 |<br>
+--------------------------------+------+--+</p>
<p>--求UV<br>
select<br>
url,count(distinct(ip))<br>
from t_access<br>
group by url</p>
<p>+--------------------------------+-----+--+<br>
| url | c1 |<br>
+--------------------------------+-----+--+<br>
| <a href="http://www.sina.com/excersize" rel="nofollow">http://www.sina.com/excersize</a> | 2 |<br>
| <a href="http://www.sina.com/job" rel="nofollow">http://www.sina.com/job</a> | 5 |<br>
| <a href="http://www.sina.com/pay" rel="nofollow">http://www.sina.com/pay</a> | 1 |<br>
| <a href="http://www.sina.com/register" rel="nofollow">http://www.sina.com/register</a> | 2 |<br>
| <a href="http://www.sina.com/stu" rel="nofollow">http://www.sina.com/stu</a> | 3 |<br>
| <a href="http://www.sina.com/teach" rel="nofollow">http://www.sina.com/teach</a> | 2 |<br>
+--------------------------------+-----+--+</p>
<p>--求每条url访问的ip中，最大的ip是哪个<br>
--PV<br>
--UV<br>
--用mapreduce实现</p>
<p>--求8月6号pv，uv<br>
select<br>
count(1),url<br>
from t_access<br>
where day = '2017-08-06'<br>
group by url</p>
<p>--求8月4号以后，每天访问<a href="http://www.sina.com/job" rel="nofollow">http://www.sina.com/job</a> 的次数，以及访问者中ip最大的<br>
select<br>
count(1),max(ip),day<br>
from t_access<br>
where day &gt; '2017-08-04'<br>
and url = '<a href="http://www.sina.com/job" rel="nofollow">http://www.sina.com/job</a>'<br>
group by day</p>
<p>select<br>
count(1),max(ip),day<br>
from t_access<br>
where url = '<a href="http://www.sina.com/job" rel="nofollow">http://www.sina.com/job</a>'<br>
group by day having day &gt; '2017-08-04'</p>
<p>--求8月4号以后，每天每个页面访问总次数，以及页面最大的ip，并且上述查询结果中访问次数大于2的<br>
select tmp.* from<br>
(select<br>
url,day,count(1) cnts,max(ip) ip<br>
from t_access<br>
group by day,url having day &gt; '2017-08-04') tmp<br>
where tmp.cnts &gt; 2</p>
<p>--每天，pv排序<br>
select<br>
url,day,count(1) cnts<br>
from t_access<br>
group by day,url<br>
order by cnts desc</p>
<p>--hive里的order by是全局排序<br>
--sort by是在map里进行排序</p>
<p>select<br>
tmp.*<br>
from<br>
(select<br>
url,day,count(1) cnts<br>
from t_access<br>
group by day,url) tmp<br>
distribute by tmp.day sort by tmp.cnts desc</p>
<p>--有如下数据<br>
+-------------+---------------+--+<br>
| t_sale.mid | t_sale.money |<br>
+-------------+---------------+--+<br>
| AA | 15.0 |<br>
| AA | 20.0 |<br>
| BB | 22.0 |<br>
| CC | 44.0 |<br>
+-------------+---------------+--+</p>
<p>--distribute by先把数据分发到各个reduce中，然后sort by在各个reduce中进行局部排序<br>
select<br>
mid,money<br>
from t_sale<br>
distribute by mid sort by money desc</p>
<p>+------+--------+--+<br>
| mid | money |<br>
+------+--------+--+<br>
| AA | 15.0 |<br>
| AA | 20.0 |<br>
| BB | 22.0 |<br>
| CC | 44.0 |<br>
+------+--------+--+</p>
<p>--cluster by mid 等于 distribute by mid sort by mid<br>
--cluster by后面不能跟desc，asc，默认的只能升序</p>
<p>--order by 是全排序，所有的数据会发送给一个reduceTask进行处理，在数据量大的时候，reduce就会超负荷<br>
select<br>
mid,money<br>
from t_sale<br>
order by money desc</p>
<p>--设置最大的reduce启动个数<br>
set hive.exec.reducers.max=10;<br>
--设置reduce的启动个数<br>
set mapreduce.job.reduce=3</p>
<p>--hive数据类型<br>
--数字类型<br>
tinyint 1byte -128到127<br>
smallint 2byte -32768到32767 char<br>
int<br>
bigint<br>
float<br>
double</p>
<p>--日期类型<br>
timestamp<br>
date</p>
<p>--字符串类型<br>
string<br>
varchar<br>
char</p>
<p>--混杂类型<br>
boolean<br>
binary 二进制</p>
<p>--复合类型<br>
--数组</p>
<p>流浪地球,吴京:吴孟达:张飞:赵云,2019-09-11<br>
我和我的祖国,葛优:黄渤:宋佳:吴京,2019-10-01</p>
<p>create table t_movie(movie_name string,actors array,show_time date)<br>
row format delimited fields terminated by ','<br>
collection items terminated by ':';</p>
<p>array_contains</p>
<p>select</p>
<p>from t_movie<br>
where array_contains(actors,'张飞')</p>
<p>size</p>
<p>select<br>
movie_name,actors,show_time,size(actors) num<br>
from t_movie</p>
<p>========================================</p>
<p>map类型<br>
1,zhangsan,father:xiaoming#mother:chentianxing#brother:shaoshuai,28<br>
2,xiaoming,father:xiaoyun#mother:xiaohong#sister:xiaoyue#wife:chentianxing,30</p>
<p>create table t_user(id int,name string,family map&lt;string,string&gt;,jage int)<br>
row format delimited fields terminated by ','<br>
collection items terminated by '#'<br>
map keys terminated by ':';</p>
<p>map_keys<br>
select<br>
id,name,map_keys(family) as relation,jage<br>
from t_user;</p>
<p>map_values<br>
select<br>
id,name,map_values(family) as relation,jage<br>
from t_user;</p>
<p>--家庭成员数量<br>
select<br>
id,name,size(family),jage<br>
from t_user</p>
<p>--有兄弟的人<br>
select</p>
<p>from t_user<br>
where array_contains(map_keys(family),'brother')</p>
<p>========================================<br>
1,zhangsan,18:man:beijing<br>
2,lisi,22:woman:shanghai</p>
<p>create table t_people(id int,name string,info structage:int,sex:string,addr:string)<br>
row format delimited fields terminated by ','<br>
collection items terminated by ':';</p>
<p>select<br>
id,name,info.age<br>
from t_people;</p>
<p>========================================</p>
<p>192,168,33,66,zhangsan,male<br>
192,168,33,77,lisi,male<br>
192,168,43,101,wangwu,female</p>
<p>create table t_people_ip(ip_seg1 string,ip_seg2 string,ip_seg3 string,ip_seg4 string,name string,sex string)<br>
row format delimited fields terminated by ',';</p>
<p>--字符串拼接函数<br>
concat<br>
select concat('abc','def')</p>
<p>concat_ws<br>
select concat_ws('.',ip_seg1,ip_seg2,ip_seg3,ip_seg4),name,sex from t_people_ip</p>
<p>===================================================================</p>
<p>--求字符串长度<br>
length<br>
select lenth('jsdfijsdkfjkdsfjkdf');</p>
<p>========================================</p>
<p>select to_date('2019-09-11 16:55:11');</p>
<p>--把字符串转换成unix时间戳<br>
select unix_timestamp('2019-09-11 11:55:11','yyyy-MM-dd HH:mm:ss');</p>
<p>--把unix时间戳转换成字符串<br>
select from_unixtime(unix_timestamp(),'yyyy-MM-dd HH:mm:ss');</p>
<p>========================================</p>
<p>--数学运算函数<br>
--四舍五入<br>
select round(5.4)<br>
--四舍五入保留2位小数<br>
select round(5.1345,2)<br>
--向上取整<br>
select ceil(5.3)<br>
--向下取整<br>
select floor(5.3)<br>
--取绝对值<br>
select abs(-5.2)<br>
--取最大值<br>
select greatest(3,4,5,6,7)<br>
--取最小值<br>
select least(3,4,5,6,7)</p>
<p>==================================================</p>
<p>1,19,a,male<br>
2,19,b,male<br>
3,22,c,female<br>
4,16,d,female<br>
5,30,e,male<br>
6,26,f,female</p>
<p>+---------------+----------------+-----------------+----------------+--+<br>
| t_student.id | t_student.age | t_student.name | t_student.sex |<br>
+---------------+----------------+-----------------+----------------+--+<br>
| 1 | 19 | a | male |<br>
| 2 | 19 | b | male |<br>
| 5 | 30 | e | male |</p>
<p>| 3 | 22 | c | female |<br>
| 4 | 16 | d | female |<br>
| 6 | 26 | f | female |</p>
<p>+---------------+----------------+-----------------+----------------+--+</p>
<p>row_number() over()</p>
<p>select<br>
id,age,name,sex,row_number() over(partition by sex order by age desc) rk<br>
from t_student</p>
<p>select<br>
tmp.*<br>
from<br>
(select<br>
id,age,name,sex,row_number() over(partition by sex order by age desc) rk<br>
from t_student) tmp<br>
where tmp.rk = 1</p>
<p>select<br>
id,age,name,sex,rank() over(partition by sex order by age desc) rk<br>
from t_student</p>
<p>select<br>
id,age,name,sex,dense_rank() over(partition by sex order by age desc) rk<br>
from t_student</p>
<p>========================================</p>
<p>A,2015-01,5<br>
A,2015-01,15<br>
B,2015-01,5<br>
A,2015-01,8<br>
B,2015-01,25<br>
A,2015-01,5<br>
C,2015-01,10<br>
C,2015-01,20<br>
A,2015-02,4<br>
A,2015-02,6<br>
C,2015-02,30<br>
C,2015-02,10<br>
B,2015-02,10<br>
B,2015-02,5<br>
A,2015-03,14<br>
A,2015-03,6<br>
B,2015-03,20<br>
B,2015-03,25<br>
C,2015-03,10<br>
C,2015-03,20</p>
<p>--求每个用户每个月的销售额和到当月位置的累计销售额<br>
create table t_saller(name string,month string,amount int)<br>
row format delimited fields terminated by ','</p>
<p>create table t_accumulate<br>
as<br>
select name,month,sum(amount) samount<br>
from t_saller<br>
group by name,month</p>
<p>+--------------------+---------------------+-----------------------+--+<br>
| t_accumulate.name | t_accumulate.month | t_accumulate.samount |<br>
+--------------------+---------------------+-----------------------+--+<br>
| A | 2015-01 | 33 |<br>
| A | 2015-02 | 10 |<br>
| A | 2015-03 | 20 |<br>
| B | 2015-01 | 30 |<br>
| B | 2015-02 | 15 |<br>
| B | 2015-03 | 45 |<br>
| C | 2015-01 | 30 |<br>
| C | 2015-02 | 40 |<br>
| C | 2015-03 | 30 |<br>
+--------------------+---------------------+-----------------------+--+<br>
--最前面一行<br>
select<br>
name,month,samount,sum(samount) over(partition by name order by month rows between unbounded preceding and current row) accumlateAmount<br>
from<br>
t_accumulate</p>
<p>| A | 2015-01 | 33 |<br>
| A | 2015-02 | 10 |<br>
| A | 2015-03 | 20 |<br>
| A | 2015-04 | 30 |<br>
| A | 2015-05 | 15 |<br>
| A | 2015-06 | 45 |<br>
| A | 2015-07 | 30 |<br>
| A | 2015-08 | 40 |<br>
| A | 2015-09 | 30 |</p>
<p>select<br>
name,month,samount,sum(samount) over(partition by name order by month rows between 2 preceding and 1 following ) accumlateAmount<br>
from<br>
t_accumulate</p>
<p>preceding |<br>
当前行 | 窗口长度<br>
following |</p>
<p>min() over() ,max() over() , avg() over()</p>
<p>========================================</p>
<p>1,zhangsan,化学:物理:数学:语文<br>
2,lisi,化学:数学:生物:生理:卫生<br>
3,wangwu,化学:语文:英语:体育:生物</p>
<p>create table t_stu_subject(id int,name string,subjects array)<br>
row format delimited fields terminated by ','<br>
collection items terminated by ':'</p>
<p>explode()</p>
<p>select explode(subjects) from t_stu_subject;</p>
<p>select distinct tmp.subs from (select explode(subjects) subs from t_stu_subject) tmp</p>
<p>========================================</p>
<p>lateral view连接函数</p>
<p>select<br>
id,name,sub<br>
from<br>
t_stu_subject lateral view explode(subjects) tmp as sub</p>
<p>+-----+-----------+------+--+<br>
| id | name | sub |<br>
+-----+-----------+------+--+<br>
| 1 | zhangsan | 化学 |<br>
| 1 | zhangsan | 物理 |<br>
| 1 | zhangsan | 数学 |<br>
| 1 | zhangsan | 语文 |<br>
| 2 | lisi | 化学 |<br>
| 2 | lisi | 数学 |<br>
| 2 | lisi | 生物 |<br>
| 2 | lisi | 生理 |<br>
| 2 | lisi | 卫生 |<br>
| 3 | wangwu | 化学 |<br>
| 3 | wangwu | 语文 |<br>
| 3 | wangwu | 英语 |<br>
| 3 | wangwu | 体育 |<br>
| 3 | wangwu | 生物 |<br>
+-----+-----------+------+--+</p>
<p>========================================</p>
<p>wordcount</p>
<p>words</p>
<p>create table words(line string)</p>
<p>line<br>
hello world hi tom and jack<br>
hello chentianxing qiaoyuan and shaoshuai<br>
hello hello hi tom and shaoshuai<br>
chentianxing love saoshuai<br>
hello love what is love how love</p>
<p>split()</p>
<p>select<br>
tmp.word,count(1) cnts<br>
from<br>
(select<br>
explode(split(line,' ')) word<br>
from words) tmp<br>
group by tmp.word order by cnts desc</p>
<p>========================================</p>
<p>--炸map<br>
select<br>
id,name,key,value<br>
from<br>
t_user<br>
lateral view explode(family) tmp as key,value</p>
<p>========================================</p>
<p>--有web系统，每天产生下列的日志文件</p>
<p>2017-09-15号的数据：<br>
192.168.33.6,hunter,2017-09-15 10:30:20,/a<br>
192.168.33.7,hunter,2017-09-15 10:30:26,/b<br>
192.168.33.6,jack,2017-09-15 10:30:27,/a<br>
192.168.33.8,tom,2017-09-15 10:30:28,/b<br>
192.168.33.9,rose,2017-09-15 10:30:30,/b<br>
192.168.33.10,julia,2017-09-15 10:30:40,/c</p>
<p>2017-09-16号的数据：<br>
192.168.33.16,hunter,2017-09-16 10:30:20,/a<br>
192.168.33.18,jerry,2017-09-16 10:30:30,/b<br>
192.168.33.26,jack,2017-09-16 10:30:40,/a<br>
192.168.33.18,polo,2017-09-16 10:30:50,/b<br>
192.168.33.39,nissan,2017-09-16 10:30:53,/b<br>
192.168.33.39,nissan,2017-09-16 10:30:55,/a<br>
192.168.33.39,nissan,2017-09-16 10:30:58,/c<br>
192.168.33.20,ford,2017-09-16 10:30:54,/c</p>
<p>2017-09-17号的数据：<br>
192.168.33.46,hunter,2017-09-17 10:30:21,/a<br>
192.168.43.18,jerry,2017-09-17 10:30:22,/b<br>
192.168.43.26,tom,2017-09-17 10:30:23,/a<br>
192.168.53.18,bmw,2017-09-17 10:30:24,/b<br>
192.168.63.39,benz,2017-09-17 10:30:25,/b<br>
192.168.33.25,haval,2017-09-17 10:30:30,/c<br>
192.168.33.10,julia,2017-09-17 10:30:40,/c</p>
<p>--统计日活跃用户</p>
<p>--统计每日新增用户</p>
<p>--统计历史用户</p>
<p>--创建日志表<br>
create table t_web_log(ip string,uname string,access_time string,url string)<br>
partitioned by (day string)<br>
row format delimited fields terminated by ',';</p>
<p>--创建日活跃用户表<br>
create table t_user_active_day<br>
like<br>
t_web_log;</p>
<p>--统计日活跃用户<br>
insert into table t_user_active_day partition(day='2017-09-15')<br>
select tmp.ip,tmp.uname,tmp.access_time,tmp.url<br>
from<br>
(select<br>
ip,uname,access_time,url,row_number() over(partition by uname order by access_time) rn<br>
from<br>
t_web_log where day='2017-09-15') tmp<br>
where rn &lt;2</p>
<p>--创建历史用户表<br>
create table t_user_history(uname string)</p>
<p>--创建日新增用户表<br>
create table t_user_new_day like t_user_active_day;</p>
<p>--统计日新增用户<br>
insert into table t_user_new_day partition(day='2017-09-15')<br>
select<br>
tua.ip,tua.uname,tua.access_time,tua.url<br>
from t_user_active_day tua<br>
left join t_user_history tuh on tua.uname = tuh.uname<br>
where tua.day = '2017-09-15' and tuh.uname IS NULL</p>
<p>--记录历史用户<br>
insert into table t_user_history<br>
select<br>
uname<br>
from t_user_new_day<br>
where day='2017-09-15'</p>
<p>insert into table t_user_active_day partition(day='2017-09-16')<br>
select tmp.ip,tmp.uname,tmp.access_time,tmp.url<br>
from<br>
(select<br>
ip,uname,access_time,url,row_number() over(partition by uname order by access_time) rn<br>
from<br>
t_web_log where day='2017-09-16') tmp<br>
where rn &lt;2</p>
<p>insert into table t_user_new_day partition(day='2017-09-16')<br>
select<br>
tua.ip,tua.uname,tua.access_time,tua.url<br>
from t_user_active_day tua<br>
left join t_user_history tuh on tua.uname = tuh.uname<br>
where tua.day = '2017-09-16' and tuh.uname IS NULL</p>
<p>insert into table t_user_history<br>
select<br>
uname<br>
from t_user_new_day<br>
where day='2017-09-16'</p>
<p>--桶表<br>
--创建桶表，按id分成三个桶<br>
create table t1(id int,name string,age int)<br>
clustered by(id) into 3 buckets<br>
row format delimited fields terminated by ',';</p>
<p>--桶表插入数据的方式<br>
create table t1_1(id int,name string,age int)<br>
row format delimited fields terminated by ',';</p>
<p>set hive.enforce.bucketing=true;<br>
set mapreduce.job.reduces=3;</p>
<p>insert into t1 select * from t1_1;</p>
<p>========================================<br>
hive自定义函数<br>
package com.bwie.hive.udf;</p>
<p>import com.alibaba.fastjson.JSONObject;<br>
import org.apache.hadoop.hive.ql.exec.UDF;</p>
<p>public class JsonUDF extends UDF {</p>
<p>public String evaluate(String json,String colum){<br>
JSONObject jsonObject = JSONObject.parseObject(json);<br>
String value = jsonObject.getString(colum);<br>
return value;<br>
}<br>
}</p>
<p>打包上传<br>
add jar /root/hivetest-1.0-SNAPSHOT.jar;<br>
create temporary function getjson as 'com.bwie.hive.udf.JsonUDF';</p>
<p>select<br>
getjson(json,'movie') as movie,getjson(json,'rate') as rate,from_unixtime(CAST(getjson(json,'timeStamp') as BIGINT),'yyyy-MM-dd HH:mm:ss') as showtime,getjson(json,'uid') as uid<br>
from t_ex_rat limit 10;</p>
<p>--创建永久函数<br>
方法一、<br>
在hive-site.xml里添加jar包<br>
<br>
hive.aux.jars.path<br>
file:///usr/local/apache-hive-1.2.2-bin/lib/hivetest-1.0-SNAPSHOT.jar<br>
<br>
疑似jar包要放到lib下才能生效<br>
create function getjson AS 'com.bwie.hive.udf.JsonUDF'</p>
<p>方法二、<br>
create function getjson as 'com.bwie.hive.udf.JsonUDF' using jar 'hdfs://hdp1703/lib/hivetest-1.0-SNAPSHOT.jar'</p>
<p>drop function getjson</p>
<p>========================================</p>
<p>create table t_employee(id int,name string,age int)<br>
partitioned by(state string,city string)<br>
row format delimited fields terminated by ','</p>
<p>create table t_employee_orgin(id int,name string,age int,state string,city string)<br>
row format delimited fields terminated by ',';</p>
<p>load data local inpath '/root/employ.txt' into table t_employee_orgin;</p>
<p>在严格模式下，多个分区一定有一个分区是固定的<br>
insert into t_employee<br>
partition(state='china',city)<br>
select id,name,age,city<br>
from t_employee_orgin<br>
where state='china'</p>
<p>--非严格模式<br>
set hive.exec.dynamic.partition.mode=nostrict;<br>
insert into t_employee<br>
partition(state,city)<br>
select id,name,age,state,city<br>
from t_employee_orgin</p>
<p>========================================</p>
<p>select to_date('2019-09-20 17:30:00')</p>
<p>unix_timestamp('2019/09/20 17:30:00','yyyy/MM/dd HH:mm:ss')</p>
<p>select from_unixtime(unix_timestamp('2019/09/20 17:30:00','yyyy/MM/dd HH:mm:ss'),'yyyy-MM-dd HH:mm:ss')</p></div>
<div style="font-size:small;margin-top:8px;float:right;"></div>

<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">评论</button>
<div class="comments" id="comments"></div>

</div>
    <div id="footer">Copyright © <span id="year"></span><a href="https://0or1world.github.io"> SHIJIE的技术记录 </a>
<p>
<span id="runday"></span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a>
</p>

<script>
if(""!=""){
    var now=new Date();
    var startSite=new Date("");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("year").innerHTML=now.getFullYear();
    if(""!=""){document.getElementById("runday").innerHTML=" • "+"网站运行"+diffDay+"天"+" • ";}
    else{document.getElementById("runday").innerHTML="网站运行"+diffDay+"天"+" • ";}
}
</script>
</div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n\n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);



function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","0or1world/0or1world.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}
</script>


</html>
