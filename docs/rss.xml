<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>shijie的技术记录</title><link>https://0or1world.github.io</link><description>技术分析与记录</description><copyright>shijie的技术记录</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://github.githubassets.com/favicons/favicon.svg</url><title>avatar</title><link>https://0or1world.github.io</link></image><lastBuildDate>Tue, 02 Jul 2024 13:28:17 +0000</lastBuildDate><managingEditor>shijie的技术记录</managingEditor><ttl>60</ttl><webMaster>shijie的技术记录</webMaster><item><title>spark运行流程</title><link>https://0or1world.github.io/post/spark-yun-xing-liu-cheng.html</link><description>### spark中基本概念&#13;
1. Application：表示你的应用程序&#13;
2. Driver：表示main()函数，创建SparkContext。</description><guid isPermaLink="true">https://0or1world.github.io/post/spark-yun-xing-liu-cheng.html</guid><pubDate>Tue, 02 Jul 2024 13:27:50 +0000</pubDate></item><item><title>spark开发调优</title><link>https://0or1world.github.io/post/spark-kai-fa-diao-you.html</link><description>## 1. 避免重复创建RDD&#13;
通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。</description><guid isPermaLink="true">https://0or1world.github.io/post/spark-kai-fa-diao-you.html</guid><pubDate>Tue, 02 Jul 2024 13:26:56 +0000</pubDate></item><item><title>hive数据倾斜原因解决方法</title><link>https://0or1world.github.io/post/hive-shu-ju-qing-xie-yuan-yin-jie-jue-fang-fa.html</link><description>### Hive倾斜之group by聚合倾斜&#13;
**原因：**&#13;
- 分组的维度过少，每个维度的值过多，导致处理某值的reduce耗时很久；&#13;
&#13;
- 对一些类型统计的时候某种类型的数据量特别多，其他的数据类型特别少。</description><guid isPermaLink="true">https://0or1world.github.io/post/hive-shu-ju-qing-xie-yuan-yin-jie-jue-fang-fa.html</guid><pubDate>Tue, 02 Jul 2024 13:26:02 +0000</pubDate></item><item><title>SPARK参数</title><link>https://0or1world.github.io/post/SPARK-can-shu.html</link><description>&#13;
&#13;
## Driver&#13;
&#13;
**spark.driver.cores**&#13;
&#13;
driver端分配的核数，默认为1，thriftserver是启动thriftserver服务的机器，资源充足的话可以尽量给多。</description><guid isPermaLink="true">https://0or1world.github.io/post/SPARK-can-shu.html</guid><pubDate>Tue, 02 Jul 2024 13:24:22 +0000</pubDate></item><item><title>hive语句大全</title><link>https://0or1world.github.io/post/hive-yu-ju-da-quan.html</link><description>hive数据库是hdfs上的文件夹，表也是文件夹，表里的数据是文件&#13;
hive建表&#13;
create table t_student(id string,name string,age int,classNo string)&#13;
row format delimited&#13;
fields terminated by ',';&#13;
&#13;
创建外部表&#13;
create external table t_a(id string,name string)&#13;
row format delimited&#13;
fields terminated by ','&#13;
location '/.../...'&#13;
外部表和内部表的区别，drop时内部表的hdfs数据一同删除，外部表的hdfs上的数据不删除&#13;
&#13;
create table t_b(id string,name string)&#13;
row format delimited&#13;
fields terminated by ','&#13;
&#13;
create table t_a(id string,name string)&#13;
row format delimited&#13;
fields terminated by ','&#13;
&#13;
--加载数据从hdfs上加载&#13;
load data inpath '/dataB/b.txt' into table t_a;&#13;
--加载数据从本地上加载&#13;
load data local inpath '/root/bb.txt' into table t_b;&#13;
&#13;
t_a&#13;
+---------+-----------+--+&#13;
| t_a.id | t_a.name |&#13;
+---------+-----------+--+&#13;
| a | 1 |&#13;
| b | 2 |&#13;
| c | 4 |&#13;
| d | 5 |&#13;
+---------+-----------+--+&#13;
&#13;
t_b&#13;
+---------+-----------+--+&#13;
| t_b.id | t_b.name |&#13;
+---------+-----------+--+&#13;
| a | xx |&#13;
| b | yy |&#13;
| d | zz |&#13;
| e | pp |&#13;
+---------+-----------+--+&#13;
&#13;
--笛卡尔积&#13;
select a.,b.&#13;
from t_a inner join t_b&#13;
&#13;
+---------+-----------+---------+-----------+--+&#13;
| t_a.id | t_a.name | t_b.id | t_b.name |&#13;
+---------+-----------+---------+-----------+--+&#13;
| a | 1 | a | xx |&#13;
| b | 2 | a | xx |&#13;
| c | 4 | a | xx |&#13;
| d | 5 | a | xx |&#13;
| a | 1 | b | yy |&#13;
| b | 2 | b | yy |&#13;
| c | 4 | b | yy |&#13;
| d | 5 | b | yy |&#13;
| a | 1 | d | zz |&#13;
| b | 2 | d | zz |&#13;
| c | 4 | d | zz |&#13;
| d | 5 | d | zz |&#13;
| a | 1 | e | pp |&#13;
| b | 2 | e | pp |&#13;
| c | 4 | e | pp |&#13;
| d | 5 | e | pp |&#13;
+---------+-----------+---------+-----------+--+&#13;
&#13;
--内连接&#13;
select * from t_a join t_b where t_a.id = t_b.id;&#13;
select * from t_a join t_b on t_a.id = t_b.id;&#13;
&#13;
--左外连接&#13;
select&#13;
a.,b.&#13;
from t_a a left join t_b b&#13;
&#13;
select&#13;
a.,b.&#13;
from t_a a left join t_b b on a.id = b.id&#13;
&#13;
+-------+---------+-------+---------+--+&#13;
| a.id | a.name | b.id | b.name |&#13;
+-------+---------+-------+---------+--+&#13;
| a | 1 | a | xx |&#13;
| b | 2 | b | yy |&#13;
| d | 5 | d | zz |&#13;
| NULL | NULL | e | pp |&#13;
+-------+---------+-------+---------+--+&#13;
&#13;
--右外连接&#13;
select&#13;
a.,b.&#13;
from t_a a right outer join t_b b&#13;
&#13;
select&#13;
a.,b.&#13;
from t_a a right outer join t_b b on a.id = b.id&#13;
+-------+---------+-------+---------+--+&#13;
| a.id | a.name | b.id | b.name |&#13;
+-------+---------+-------+---------+--+&#13;
| a | 1 | a | xx |&#13;
| b | 2 | b | yy |&#13;
| d | 5 | d | zz |&#13;
| NULL | NULL | e | pp |&#13;
+-------+---------+-------+---------+--+&#13;
&#13;
--全外连接&#13;
select&#13;
a.,b.&#13;
from t_a a full outer join t_b b on a.id = b.id&#13;
&#13;
+-------+---------+-------+---------+--+&#13;
| a.id | a.name | b.id | b.name |&#13;
+-------+---------+-------+---------+--+&#13;
| a | 1 | a | xx |&#13;
| b | 2 | b | yy |&#13;
| c | 4 | NULL | NULL |&#13;
| d | 5 | d | zz |&#13;
| NULL | NULL | e | pp |&#13;
+-------+---------+-------+---------+--+&#13;
&#13;
--左半连接 相当于指定条件的内连接，但是只显示左边的表数据&#13;
select&#13;
a.*&#13;
from t_a a left semi join t_b b on a.id = b.id&#13;
&#13;
+-------+---------+--+&#13;
| a.id | a.name |&#13;
+-------+---------+--+&#13;
| a | 1 |&#13;
| b | 2 |&#13;
| d | 5 |&#13;
+-------+---------+--+&#13;
&#13;
--例如有这些数据&#13;
vi access.log.0804&#13;
192.168.33.3,http://www.sina.com/stu,2017-08-04 15:30:20&#13;
192.168.33.3,http://www.sina.com/teach,2017-08-04 15:35:20&#13;
192.168.33.4,http://www.sina.com/stu,2017-08-04 15:30:20&#13;
192.168.33.4,http://www.sina.com/job,2017-08-04 16:30:20&#13;
192.168.33.5,http://www.sina.com/job,2017-08-04 15:40:20&#13;
&#13;
vi access.log.0805&#13;
192.168.33.3,http://www.sina.com/stu,2017-08-05 15:30:20&#13;
192.168.44.3,http://www.sina.com/teach,2017-08-05 15:35:20&#13;
192.168.33.44,http://www.sina.com/stu,2017-08-05 15:30:20&#13;
192.168.33.46,http://www.sina.com/job,2017-08-05 16:30:20&#13;
192.168.33.55,http://www.sina.com/job,2017-08-05 15:40:20&#13;
&#13;
vi access.log.0806&#13;
192.168.133.3,http://www.sina.com/register,2017-08-06 15:30:20&#13;
192.168.111.3,http://www.sina.com/register,2017-08-06 15:35:20&#13;
192.168.34.44,http://www.sina.com/pay,2017-08-06 15:30:20&#13;
192.168.33.46,http://www.sina.com/excersize,2017-08-06 16:30:20&#13;
192.168.33.55,http://www.sina.com/job,2017-08-06 15:40:20&#13;
192.168.33.46,http://www.sina.com/excersize,2017-08-06 16:30:20&#13;
192.168.33.25,http://www.sina.com/job,2017-08-06 15:40:20&#13;
192.168.33.36,http://www.sina.com/excersize,2017-08-06 16:30:20&#13;
192.168.33.55,http://www.sina.com/job,2017-08-06 15:40:20&#13;
&#13;
--建分区表&#13;
create table t_access(ip string,url string,access_time string)&#13;
partitioned by (day string)&#13;
row format delimited&#13;
fields terminated by ','&#13;
&#13;
--导入分区表数据&#13;
load data local inpath '/root/access.log.0806' into table t_access partition(day='2017-08-06');&#13;
&#13;
--查看当前分区情况&#13;
show partitions t_access;&#13;
&#13;
--求每条url访问的次数&#13;
select&#13;
url,count(1)&#13;
from t_access&#13;
group by url;&#13;
&#13;
--求每条url访问的ip中，最大的ip是哪个&#13;
select&#13;
url,max(ip)&#13;
from t_access&#13;
group by url;&#13;
&#13;
--求每个ip访问同一个页面的记录中，最晚的一条&#13;
select&#13;
ip,url,max(access_time)&#13;
from t_access&#13;
group by ip,url&#13;
&#13;
--求PV&#13;
select&#13;
url,count(1)&#13;
from t_access&#13;
group by url&#13;
&#13;
+--------------------------------+------+--+&#13;
| url | _c1 |&#13;
+--------------------------------+------+--+&#13;
| http://www.sina.com/excersize | 3 |&#13;
| http://www.sina.com/job | 7 |&#13;
| http://www.sina.com/pay | 1 |&#13;
| http://www.sina.com/register | 2 |&#13;
| http://www.sina.com/stu | 4 |&#13;
| http://www.sina.com/teach | 2 |&#13;
+--------------------------------+------+--+&#13;
&#13;
--求UV&#13;
select&#13;
url,count(distinct(ip))&#13;
from t_access&#13;
group by url&#13;
&#13;
+--------------------------------+-----+--+&#13;
| url | c1 |&#13;
+--------------------------------+-----+--+&#13;
| http://www.sina.com/excersize | 2 |&#13;
| http://www.sina.com/job | 5 |&#13;
| http://www.sina.com/pay | 1 |&#13;
| http://www.sina.com/register | 2 |&#13;
| http://www.sina.com/stu | 3 |&#13;
| http://www.sina.com/teach | 2 |&#13;
+--------------------------------+-----+--+&#13;
&#13;
--求每条url访问的ip中，最大的ip是哪个&#13;
--PV&#13;
--UV&#13;
--用mapreduce实现&#13;
&#13;
--求8月6号pv，uv&#13;
select&#13;
count(1),url&#13;
from t_access&#13;
where day = '2017-08-06'&#13;
group by url&#13;
&#13;
--求8月4号以后，每天访问http://www.sina.com/job 的次数，以及访问者中ip最大的&#13;
select&#13;
count(1),max(ip),day&#13;
from t_access&#13;
where day &gt; '2017-08-04'&#13;
and url = 'http://www.sina.com/job'&#13;
group by day&#13;
&#13;
select&#13;
count(1),max(ip),day&#13;
from t_access&#13;
where url = 'http://www.sina.com/job'&#13;
group by day having day &gt; '2017-08-04'&#13;
&#13;
--求8月4号以后，每天每个页面访问总次数，以及页面最大的ip，并且上述查询结果中访问次数大于2的&#13;
select tmp.* from&#13;
(select&#13;
url,day,count(1) cnts,max(ip) ip&#13;
from t_access&#13;
group by day,url having day &gt; '2017-08-04') tmp&#13;
where tmp.cnts &gt; 2&#13;
&#13;
--每天，pv排序&#13;
select&#13;
url,day,count(1) cnts&#13;
from t_access&#13;
group by day,url&#13;
order by cnts desc&#13;
&#13;
--hive里的order by是全局排序&#13;
--sort by是在map里进行排序&#13;
&#13;
select&#13;
tmp.*&#13;
from&#13;
(select&#13;
url,day,count(1) cnts&#13;
from t_access&#13;
group by day,url) tmp&#13;
distribute by tmp.day sort by tmp.cnts desc&#13;
&#13;
--有如下数据&#13;
+-------------+---------------+--+&#13;
| t_sale.mid | t_sale.money |&#13;
+-------------+---------------+--+&#13;
| AA | 15.0 |&#13;
| AA | 20.0 |&#13;
| BB | 22.0 |&#13;
| CC | 44.0 |&#13;
+-------------+---------------+--+&#13;
&#13;
--distribute by先把数据分发到各个reduce中，然后sort by在各个reduce中进行局部排序&#13;
select&#13;
mid,money&#13;
from t_sale&#13;
distribute by mid sort by money desc&#13;
&#13;
+------+--------+--+&#13;
| mid | money |&#13;
+------+--------+--+&#13;
| AA | 15.0 |&#13;
| AA | 20.0 |&#13;
| BB | 22.0 |&#13;
| CC | 44.0 |&#13;
+------+--------+--+&#13;
&#13;
--cluster by mid 等于 distribute by mid sort by mid&#13;
--cluster by后面不能跟desc，asc，默认的只能升序&#13;
&#13;
--order by 是全排序，所有的数据会发送给一个reduceTask进行处理，在数据量大的时候，reduce就会超负荷&#13;
select&#13;
mid,money&#13;
from t_sale&#13;
order by money desc&#13;
&#13;
--设置最大的reduce启动个数&#13;
set hive.exec.reducers.max=10;&#13;
--设置reduce的启动个数&#13;
set mapreduce.job.reduce=3&#13;
&#13;
--hive数据类型&#13;
--数字类型&#13;
tinyint 1byte -128到127&#13;
smallint 2byte -32768到32767 char&#13;
int&#13;
bigint&#13;
float&#13;
double&#13;
&#13;
--日期类型&#13;
timestamp&#13;
date&#13;
&#13;
--字符串类型&#13;
string&#13;
varchar&#13;
char&#13;
&#13;
--混杂类型&#13;
boolean&#13;
binary 二进制&#13;
&#13;
--复合类型&#13;
--数组&#13;
&#13;
流浪地球,吴京:吴孟达:张飞:赵云,2019-09-11&#13;
我和我的祖国,葛优:黄渤:宋佳:吴京,2019-10-01&#13;
&#13;
create table t_movie(movie_name string,actors array&lt;string&gt;,show_time date)&#13;
row format delimited fields terminated by ','&#13;
collection items terminated by ':';&#13;
&#13;
array_contains&#13;
&#13;
select&#13;
&#13;
from t_movie&#13;
where array_contains(actors,'张飞')&#13;
&#13;
size&#13;
&#13;
select&#13;
movie_name,actors,show_time,size(actors) num&#13;
from t_movie&#13;
&#13;
========================================&#13;
&#13;
map类型&#13;
1,zhangsan,father:xiaoming#mother:chentianxing#brother:shaoshuai,28&#13;
2,xiaoming,father:xiaoyun#mother:xiaohong#sister:xiaoyue#wife:chentianxing,30&#13;
&#13;
create table t_user(id int,name string,family map&lt;string,string&gt;,jage int)&#13;
row format delimited fields terminated by ','&#13;
collection items terminated by '#'&#13;
map keys terminated by ':';&#13;
&#13;
map_keys&#13;
select&#13;
id,name,map_keys(family) as relation,jage&#13;
from t_user;&#13;
&#13;
map_values&#13;
select&#13;
id,name,map_values(family) as relation,jage&#13;
from t_user;&#13;
&#13;
--家庭成员数量&#13;
select&#13;
id,name,size(family),jage&#13;
from t_user&#13;
&#13;
--有兄弟的人&#13;
select&#13;
&#13;
from t_user&#13;
where array_contains(map_keys(family),'brother')&#13;
&#13;
========================================&#13;
1,zhangsan,18:man:beijing&#13;
2,lisi,22:woman:shanghai&#13;
&#13;
create table t_people(id int,name string,info struct&lt;age:int,sex:string,addr:string&gt;)&#13;
row format delimited fields terminated by ','&#13;
collection items terminated by ':';&#13;
&#13;
select&#13;
id,name,info.age&#13;
from t_people;&#13;
&#13;
========================================&#13;
&#13;
192,168,33,66,zhangsan,male&#13;
192,168,33,77,lisi,male&#13;
192,168,43,101,wangwu,female&#13;
&#13;
create table t_people_ip(ip_seg1 string,ip_seg2 string,ip_seg3 string,ip_seg4 string,name string,sex string)&#13;
row format delimited fields terminated by ',';&#13;
&#13;
--字符串拼接函数&#13;
concat&#13;
select concat('abc','def')&#13;
&#13;
concat_ws&#13;
select concat_ws('.',ip_seg1,ip_seg2,ip_seg3,ip_seg4),name,sex from t_people_ip&#13;
&#13;
===================================================================&#13;
&#13;
--求字符串长度&#13;
length&#13;
select lenth('jsdfijsdkfjkdsfjkdf');&#13;
&#13;
========================================&#13;
&#13;
select to_date('2019-09-11 16:55:11');&#13;
&#13;
--把字符串转换成unix时间戳&#13;
select unix_timestamp('2019-09-11 11:55:11','yyyy-MM-dd HH:mm:ss');&#13;
&#13;
--把unix时间戳转换成字符串&#13;
select from_unixtime(unix_timestamp(),'yyyy-MM-dd HH:mm:ss');&#13;
&#13;
========================================&#13;
&#13;
--数学运算函数&#13;
--四舍五入&#13;
select round(5.4)&#13;
--四舍五入保留2位小数&#13;
select round(5.1345,2)&#13;
--向上取整&#13;
select ceil(5.3)&#13;
--向下取整&#13;
select floor(5.3)&#13;
--取绝对值&#13;
select abs(-5.2)&#13;
--取最大值&#13;
select greatest(3,4,5,6,7)&#13;
--取最小值&#13;
select least(3,4,5,6,7)&#13;
&#13;
==================================================&#13;
&#13;
1,19,a,male&#13;
2,19,b,male&#13;
3,22,c,female&#13;
4,16,d,female&#13;
5,30,e,male&#13;
6,26,f,female&#13;
&#13;
+---------------+----------------+-----------------+----------------+--+&#13;
| t_student.id | t_student.age | t_student.name | t_student.sex |&#13;
+---------------+----------------+-----------------+----------------+--+&#13;
| 1 | 19 | a | male |&#13;
| 2 | 19 | b | male |&#13;
| 5 | 30 | e | male |&#13;
&#13;
| 3 | 22 | c | female |&#13;
| 4 | 16 | d | female |&#13;
| 6 | 26 | f | female |&#13;
&#13;
+---------------+----------------+-----------------+----------------+--+&#13;
&#13;
row_number() over()&#13;
&#13;
select&#13;
id,age,name,sex,row_number() over(partition by sex order by age desc) rk&#13;
from t_student&#13;
&#13;
select&#13;
tmp.*&#13;
from&#13;
(select&#13;
id,age,name,sex,row_number() over(partition by sex order by age desc) rk&#13;
from t_student) tmp&#13;
where tmp.rk = 1&#13;
&#13;
select&#13;
id,age,name,sex,rank() over(partition by sex order by age desc) rk&#13;
from t_student&#13;
&#13;
select&#13;
id,age,name,sex,dense_rank() over(partition by sex order by age desc) rk&#13;
from t_student&#13;
&#13;
========================================&#13;
&#13;
A,2015-01,5&#13;
A,2015-01,15&#13;
B,2015-01,5&#13;
A,2015-01,8&#13;
B,2015-01,25&#13;
A,2015-01,5&#13;
C,2015-01,10&#13;
C,2015-01,20&#13;
A,2015-02,4&#13;
A,2015-02,6&#13;
C,2015-02,30&#13;
C,2015-02,10&#13;
B,2015-02,10&#13;
B,2015-02,5&#13;
A,2015-03,14&#13;
A,2015-03,6&#13;
B,2015-03,20&#13;
B,2015-03,25&#13;
C,2015-03,10&#13;
C,2015-03,20&#13;
&#13;
--求每个用户每个月的销售额和到当月位置的累计销售额&#13;
create table t_saller(name string,month string,amount int)&#13;
row format delimited fields terminated by ','&#13;
&#13;
create table t_accumulate&#13;
as&#13;
select name,month,sum(amount) samount&#13;
from t_saller&#13;
group by name,month&#13;
&#13;
+--------------------+---------------------+-----------------------+--+&#13;
| t_accumulate.name | t_accumulate.month | t_accumulate.samount |&#13;
+--------------------+---------------------+-----------------------+--+&#13;
| A | 2015-01 | 33 |&#13;
| A | 2015-02 | 10 |&#13;
| A | 2015-03 | 20 |&#13;
| B | 2015-01 | 30 |&#13;
| B | 2015-02 | 15 |&#13;
| B | 2015-03 | 45 |&#13;
| C | 2015-01 | 30 |&#13;
| C | 2015-02 | 40 |&#13;
| C | 2015-03 | 30 |&#13;
+--------------------+---------------------+-----------------------+--+&#13;
--最前面一行&#13;
select&#13;
name,month,samount,sum(samount) over(partition by name order by month rows between unbounded preceding and current row) accumlateAmount&#13;
from&#13;
t_accumulate&#13;
&#13;
| A | 2015-01 | 33 |&#13;
| A | 2015-02 | 10 |&#13;
| A | 2015-03 | 20 |&#13;
| A | 2015-04 | 30 |&#13;
| A | 2015-05 | 15 |&#13;
| A | 2015-06 | 45 |&#13;
| A | 2015-07 | 30 |&#13;
| A | 2015-08 | 40 |&#13;
| A | 2015-09 | 30 |&#13;
&#13;
select&#13;
name,month,samount,sum(samount) over(partition by name order by month rows between 2 preceding and 1 following ) accumlateAmount&#13;
from&#13;
t_accumulate&#13;
&#13;
preceding |&#13;
当前行 | 窗口长度&#13;
following |&#13;
&#13;
min() over() ,max() over() , avg() over()&#13;
&#13;
========================================&#13;
&#13;
1,zhangsan,化学:物理:数学:语文&#13;
2,lisi,化学:数学:生物:生理:卫生&#13;
3,wangwu,化学:语文:英语:体育:生物&#13;
&#13;
create table t_stu_subject(id int,name string,subjects array&lt;string&gt;)&#13;
row format delimited fields terminated by ','&#13;
collection items terminated by ':'&#13;
&#13;
explode()&#13;
&#13;
select explode(subjects) from t_stu_subject;&#13;
&#13;
select distinct tmp.subs from (select explode(subjects) subs from t_stu_subject) tmp&#13;
&#13;
========================================&#13;
&#13;
lateral view连接函数&#13;
&#13;
select&#13;
id,name,sub&#13;
from&#13;
t_stu_subject lateral view explode(subjects) tmp as sub&#13;
&#13;
+-----+-----------+------+--+&#13;
| id | name | sub |&#13;
+-----+-----------+------+--+&#13;
| 1 | zhangsan | 化学 |&#13;
| 1 | zhangsan | 物理 |&#13;
| 1 | zhangsan | 数学 |&#13;
| 1 | zhangsan | 语文 |&#13;
| 2 | lisi | 化学 |&#13;
| 2 | lisi | 数学 |&#13;
| 2 | lisi | 生物 |&#13;
| 2 | lisi | 生理 |&#13;
| 2 | lisi | 卫生 |&#13;
| 3 | wangwu | 化学 |&#13;
| 3 | wangwu | 语文 |&#13;
| 3 | wangwu | 英语 |&#13;
| 3 | wangwu | 体育 |&#13;
| 3 | wangwu | 生物 |&#13;
+-----+-----------+------+--+&#13;
&#13;
========================================&#13;
&#13;
wordcount&#13;
&#13;
words&#13;
&#13;
create table words(line string)&#13;
&#13;
line&#13;
hello world hi tom and jack&#13;
hello chentianxing qiaoyuan and shaoshuai&#13;
hello hello hi tom and shaoshuai&#13;
chentianxing love saoshuai&#13;
hello love what is love how love&#13;
&#13;
split()&#13;
&#13;
select&#13;
tmp.word,count(1) cnts&#13;
from&#13;
(select&#13;
explode(split(line,' ')) word&#13;
from words) tmp&#13;
group by tmp.word order by cnts desc&#13;
&#13;
========================================&#13;
&#13;
--炸map&#13;
select&#13;
id,name,key,value&#13;
from&#13;
t_user&#13;
lateral view explode(family) tmp as key,value&#13;
&#13;
========================================&#13;
&#13;
--有web系统，每天产生下列的日志文件&#13;
&#13;
2017-09-15号的数据：&#13;
192.168.33.6,hunter,2017-09-15 10:30:20,/a&#13;
192.168.33.7,hunter,2017-09-15 10:30:26,/b&#13;
192.168.33.6,jack,2017-09-15 10:30:27,/a&#13;
192.168.33.8,tom,2017-09-15 10:30:28,/b&#13;
192.168.33.9,rose,2017-09-15 10:30:30,/b&#13;
192.168.33.10,julia,2017-09-15 10:30:40,/c&#13;
&#13;
2017-09-16号的数据：&#13;
192.168.33.16,hunter,2017-09-16 10:30:20,/a&#13;
192.168.33.18,jerry,2017-09-16 10:30:30,/b&#13;
192.168.33.26,jack,2017-09-16 10:30:40,/a&#13;
192.168.33.18,polo,2017-09-16 10:30:50,/b&#13;
192.168.33.39,nissan,2017-09-16 10:30:53,/b&#13;
192.168.33.39,nissan,2017-09-16 10:30:55,/a&#13;
192.168.33.39,nissan,2017-09-16 10:30:58,/c&#13;
192.168.33.20,ford,2017-09-16 10:30:54,/c&#13;
&#13;
2017-09-17号的数据：&#13;
192.168.33.46,hunter,2017-09-17 10:30:21,/a&#13;
192.168.43.18,jerry,2017-09-17 10:30:22,/b&#13;
192.168.43.26,tom,2017-09-17 10:30:23,/a&#13;
192.168.53.18,bmw,2017-09-17 10:30:24,/b&#13;
192.168.63.39,benz,2017-09-17 10:30:25,/b&#13;
192.168.33.25,haval,2017-09-17 10:30:30,/c&#13;
192.168.33.10,julia,2017-09-17 10:30:40,/c&#13;
&#13;
--统计日活跃用户&#13;
&#13;
--统计每日新增用户&#13;
&#13;
--统计历史用户&#13;
&#13;
--创建日志表&#13;
create table t_web_log(ip string,uname string,access_time string,url string)&#13;
partitioned by (day string)&#13;
row format delimited fields terminated by ',';&#13;
&#13;
--创建日活跃用户表&#13;
create table t_user_active_day&#13;
like&#13;
t_web_log;&#13;
&#13;
--统计日活跃用户&#13;
insert into table t_user_active_day partition(day='2017-09-15')&#13;
select tmp.ip,tmp.uname,tmp.access_time,tmp.url&#13;
from&#13;
(select&#13;
ip,uname,access_time,url,row_number() over(partition by uname order by access_time) rn&#13;
from&#13;
t_web_log where day='2017-09-15') tmp&#13;
where rn &lt;2&#13;
&#13;
--创建历史用户表&#13;
create table t_user_history(uname string)&#13;
&#13;
--创建日新增用户表&#13;
create table t_user_new_day like t_user_active_day;&#13;
&#13;
--统计日新增用户&#13;
insert into table t_user_new_day partition(day='2017-09-15')&#13;
select&#13;
tua.ip,tua.uname,tua.access_time,tua.url&#13;
from t_user_active_day tua&#13;
left join t_user_history tuh on tua.uname = tuh.uname&#13;
where tua.day = '2017-09-15' and tuh.uname IS NULL&#13;
&#13;
--记录历史用户&#13;
insert into table t_user_history&#13;
select&#13;
uname&#13;
from t_user_new_day&#13;
where day='2017-09-15'&#13;
&#13;
insert into table t_user_active_day partition(day='2017-09-16')&#13;
select tmp.ip,tmp.uname,tmp.access_time,tmp.url&#13;
from&#13;
(select&#13;
ip,uname,access_time,url,row_number() over(partition by uname order by access_time) rn&#13;
from&#13;
t_web_log where day='2017-09-16') tmp&#13;
where rn &lt;2&#13;
&#13;
insert into table t_user_new_day partition(day='2017-09-16')&#13;
select&#13;
tua.ip,tua.uname,tua.access_time,tua.url&#13;
from t_user_active_day tua&#13;
left join t_user_history tuh on tua.uname = tuh.uname&#13;
where tua.day = '2017-09-16' and tuh.uname IS NULL&#13;
&#13;
insert into table t_user_history&#13;
select&#13;
uname&#13;
from t_user_new_day&#13;
where day='2017-09-16'&#13;
&#13;
--桶表&#13;
--创建桶表，按id分成三个桶&#13;
create table t1(id int,name string,age int)&#13;
clustered by(id) into 3 buckets&#13;
row format delimited fields terminated by ',';&#13;
&#13;
--桶表插入数据的方式&#13;
create table t1_1(id int,name string,age int)&#13;
row format delimited fields terminated by ',';&#13;
&#13;
set hive.enforce.bucketing=true;&#13;
set mapreduce.job.reduces=3;&#13;
&#13;
insert into t1 select * from t1_1;&#13;
&#13;
========================================&#13;
hive自定义函数&#13;
package com.bwie.hive.udf;&#13;
&#13;
import com.alibaba.fastjson.JSONObject;&#13;
import org.apache.hadoop.hive.ql.exec.UDF;&#13;
&#13;
public class JsonUDF extends UDF {&#13;
&#13;
public String evaluate(String json,String colum){&#13;
    JSONObject jsonObject = JSONObject.parseObject(json);&#13;
    String value = jsonObject.getString(colum);&#13;
    return value;&#13;
}&#13;
}&#13;
&#13;
打包上传&#13;
add jar /root/hivetest-1.0-SNAPSHOT.jar;&#13;
create temporary function getjson as 'com.bwie.hive.udf.JsonUDF';&#13;
&#13;
select&#13;
getjson(json,'movie') as movie,getjson(json,'rate') as rate,from_unixtime(CAST(getjson(json,'timeStamp') as BIGINT),'yyyy-MM-dd HH:mm:ss') as showtime,getjson(json,'uid') as uid&#13;
from t_ex_rat limit 10;&#13;
&#13;
--创建永久函数&#13;
方法一、&#13;
在hive-site.xml里添加jar包&#13;
&lt;property&gt;&#13;
&lt;name&gt;hive.aux.jars.path&lt;/name&gt;&#13;
&lt;value&gt;file:///usr/local/apache-hive-1.2.2-bin/lib/hivetest-1.0-SNAPSHOT.jar&lt;/value&gt;&#13;
&lt;/property&gt;&#13;
疑似jar包要放到lib下才能生效&#13;
create function getjson AS 'com.bwie.hive.udf.JsonUDF'&#13;
&#13;
方法二、&#13;
create function getjson as 'com.bwie.hive.udf.JsonUDF' using jar 'hdfs://hdp1703/lib/hivetest-1.0-SNAPSHOT.jar'&#13;
&#13;
drop function getjson&#13;
&#13;
========================================&#13;
&#13;
create table t_employee(id int,name string,age int)&#13;
partitioned by(state string,city string)&#13;
row format delimited fields terminated by ','&#13;
&#13;
create table t_employee_orgin(id int,name string,age int,state string,city string)&#13;
row format delimited fields terminated by ',';&#13;
&#13;
load data local inpath '/root/employ.txt' into table t_employee_orgin;&#13;
&#13;
在严格模式下，多个分区一定有一个分区是固定的&#13;
insert into t_employee&#13;
partition(state='china',city)&#13;
select id,name,age,city&#13;
from t_employee_orgin&#13;
where state='china'&#13;
&#13;
--非严格模式&#13;
set hive.exec.dynamic.partition.mode=nostrict;&#13;
insert into t_employee&#13;
partition(state,city)&#13;
select id,name,age,state,city&#13;
from t_employee_orgin&#13;
&#13;
========================================&#13;
&#13;
select to_date('2019-09-20 17:30:00')&#13;
&#13;
unix_timestamp('2019/09/20 17:30:00','yyyy/MM/dd HH:mm:ss')&#13;
&#13;
select from_unixtime(unix_timestamp('2019/09/20 17:30:00','yyyy/MM/dd HH:mm:ss'),'yyyy-MM-dd HH:mm:ss')。</description><guid isPermaLink="true">https://0or1world.github.io/post/hive-yu-ju-da-quan.html</guid><pubDate>Tue, 02 Jul 2024 13:05:47 +0000</pubDate></item></channel></rss>