<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>shijie的技术记录</title><link>https://0or1world.github.io</link><description>技术分析与记录</description><copyright>shijie的技术记录</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://github.githubassets.com/favicons/favicon.svg</url><title>avatar</title><link>https://0or1world.github.io</link></image><lastBuildDate>Tue, 02 Jul 2024 13:33:33 +0000</lastBuildDate><managingEditor>shijie的技术记录</managingEditor><ttl>60</ttl><webMaster>shijie的技术记录</webMaster><item><title>Flink之Keyed State</title><link>https://0or1world.github.io/post/Flink-zhi-Keyed%20State.html</link><description>**源码解析**&#13;
``` public class CountWindowAverage extends RichFlatMapFunction&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; {&#13;
&#13;
    /**&#13;
     * The ValueState handle. The first field is the count, the second field a running sum.&#13;
     */&#13;
    /*&#13;
      ValueState 运行时保存在Taskmanager内存里&#13;
      checkkpoint的时候,把state保存在远端文件系统里&#13;
      当flink开启checkkpoint的时候,默认state保存在taskmanagerd    的内存里checkkpoint保存在jobmanager&#13;
      生产模式,state保存在taskManager的rocksdb文件系统里,checkkpoint保存在hdfs里&#13;
    */ &#13;
    private transient ValueState&lt;Tuple2&lt;Long, Long&gt;&gt; sum;&#13;
&#13;
    @Override&#13;
    public void flatMap(Tuple2&lt;Long, Long&gt; input, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out) throws Exception {&#13;
&#13;
        // sum可以访问里面的数据&#13;
        Tuple2&lt;Long, Long&gt; currentSum = sum.value();&#13;
&#13;
        // 元组下标0的+1&#13;
        currentSum.f0 += 1;&#13;
&#13;
        // 元组下标1 = (传入元组下标1的+1)&#13;
        currentSum.f1 += input.f1;&#13;
&#13;
        // 将Valuestate更新到sum里&#13;
        sum.update(currentSum);&#13;
&#13;
        // 当currentSum.f0 &gt;= 2时输出平均数&#13;
        if (currentSum.f0 &gt;= 2) {&#13;
            out.collect(new Tuple2&lt;&gt;(input.f0, currentSum.f1 / currentSum.f0));&#13;
            //清空sum&#13;
           // sum.clear();&#13;
        }&#13;
    }&#13;
&#13;
    @Override&#13;
    public void open(Configuration config) {&#13;
        ValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor =&#13;
                      //定义ValueState描述&#13;
                new ValueStateDescriptor&lt;&gt;(&#13;
                        'average', // the state name&#13;
                        TypeInformation.of(new TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() {}), // 类型是Tuple2&#13;
                        Tuple2.of(0L, 0L)); // 默认值&#13;
                  //通过描述获得sumstate&#13;
        sum = getRuntimeContext().getState(descriptor);&#13;
    }&#13;
}&#13;
&#13;
// 例子&#13;
        env.enableCheckpointing(2000);&#13;
&#13;
&#13;
&#13;
// advanced options:&#13;
&#13;
// set mode to exactly-once (this is the default)&#13;
        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);&#13;
&#13;
// make sure 500 ms of progress happen between checkpoints&#13;
        env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);&#13;
&#13;
// checkpoints have to complete within one minute, or are discarded&#13;
        env.getCheckpointConfig().setCheckpointTimeout(60000);&#13;
&#13;
// allow only one checkpoint to be in progress at the same time&#13;
        env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);&#13;
&#13;
// enable externalized checkpoints which are retained after job cancellation&#13;
        env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#13;
&#13;
        //无重启策略&#13;
        //env.setRestartStrategy(RestartStrategies.noRestart());&#13;
&#13;
&#13;
// allow job recovery fallback to checkpoint when there is a more recent savepoint&#13;
       env.setStateBackend(new FsStateBackend('file:///C:\\Users\\19191\\Desktop\\test'));&#13;
&#13;
env.fromElements(Tuple2.of(1L, 3L), Tuple2.of(1L, 5L), Tuple2.of(1L, 7L), Tuple2.of(1L, 4L), Tuple2.of(1L, 2L))&#13;
        .keyBy(0)&#13;
                      //新建&#13;
        .flatMap(new CountWindowAverage())&#13;
        .print();&#13;
&#13;
// the printed output will be (1,4) and (1,5)&#13;
//打包运行&#13;
bin/flink run -c [全限定名]  [jar包位置]&#13;
&#13;
//恢复&#13;
$ bin/flink run -s :checkpointMetaDataPath [matedata文件路径] -c  [全限定名]  [jar包位置]。</description><guid isPermaLink="true">https://0or1world.github.io/post/Flink-zhi-Keyed%20State.html</guid><pubDate>Tue, 02 Jul 2024 13:33:00 +0000</pubDate></item><item><title>Flink之Watermarks</title><link>https://0or1world.github.io/post/Flink-zhi-Watermarks.html</link><description>**源码刨析** **:**&#13;
设置水印时间防止数据乱序&#13;
```&#13;
public class BoundedOutOfOrdernessGenerator implements AssignerWithPeriodicWatermarks&lt;MyEvent&gt; {&#13;
&#13;
    private final long maxOutOfOrderness = 10000; //最大允许乱序时间&#13;
&#13;
    private long currentMaxTimestamp;//现在最大的时间戳&#13;
&#13;
    @Override&#13;
    //extractTimestamp抽取数据的时间戳&#13;
    public long extractTimestamp(MyEvent element, long previousElementTimestamp) {&#13;
        //从element获取时间&#13;
        long timestamp = element.getCreationTime();&#13;
        //从timestamp和最大时间戳取最大时间&#13;
        currentMaxTimestamp = Math.max(timestamp, currentMaxTimestamp);&#13;
        return timestamp;&#13;
    }&#13;
&#13;
    @Override&#13;
    //getCurrentWatermark得到当前水印时间(100ms调用一次)&#13;
    public Watermark getCurrentWatermark() {&#13;
        // 水印=当前最大的时间戳 - 最大允许乱序时间&#13;
        return new Watermark(currentMaxTimestamp - maxOutOfOrderness);&#13;
    }&#13;
}&#13;
```&#13;
**模拟Watermarks** **:**&#13;
```&#13;
public static void main(String[] args) throws Exception {&#13;
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&#13;
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);&#13;
&#13;
        env.setParallelism(1);&#13;
&#13;
        DataStreamSource&lt;String&gt; text = env.socketTextStream('localhost', 8888, '\n');&#13;
&#13;
        SingleOutputStreamOperator&lt;Tuple2&lt;String, Long&gt;&gt; inputmap = text.map(new MapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() {&#13;
            @Override&#13;
            public Tuple2&lt;String, Long&gt; map(String value) throws Exception {&#13;
                String[] arr = value.split(',');&#13;
                return new Tuple2&lt;String, Long&gt;(arr[0], Long.parseLong(arr[1]));&#13;
            }&#13;
        });&#13;
&#13;
        SingleOutputStreamOperator&lt;Tuple2&lt;String, Long&gt;&gt; watermarkStream = inputmap.assignTimestampsAndWatermarks(new AssignerWithPeriodicWatermarks&lt;Tuple2&lt;String, Long&gt;&gt;() {&#13;
&#13;
&#13;
            SimpleDateFormat sdf = new SimpleDateFormat('yyyy-MM-dd HH:mm:ss.SSS');&#13;
            Long currentMaxTimestamp = 0L;&#13;
            final Long maxOutOfOrderness = 10000L;// 最大允许的乱序时间是10s&#13;
&#13;
            //默认100ms被调用一次&#13;
            @Nullable&#13;
            @Override&#13;
            public Watermark getCurrentWatermark() {&#13;
                return new Watermark(currentMaxTimestamp - maxOutOfOrderness);&#13;
            }&#13;
&#13;
            //定义如何提取timestamp&#13;
            @Override&#13;
            public long extractTimestamp(Tuple2&lt;String, Long&gt; element, long previousElementTimestamp) {&#13;
                long timestamp = element.f1;&#13;
                currentMaxTimestamp = Math.max(timestamp, currentMaxTimestamp);&#13;
                long id = Thread.currentThread().getId();&#13;
                System.out.println('currentThreadId:' + id + ',key:' + element.f0 + ',eventtime:[' + element.f1 + '|' + sdf.format(element.f1) + '],currentMaxTimestamp:[' + currentMaxTimestamp + '|' +&#13;
                        sdf.format(currentMaxTimestamp) + '],watermark:[' + getCurrentWatermark().getTimestamp() + '|' + sdf.format(getCurrentWatermark().getTimestamp()) + ']');&#13;
                return timestamp;&#13;
            }&#13;
        });&#13;
&#13;
        //保存被丢弃的数据&#13;
        OutputTag&lt;Tuple2&lt;String, Long&gt;&gt; outputTag = new OutputTag&lt;Tuple2&lt;String, Long&gt;&gt;('late-data'){};&#13;
        SingleOutputStreamOperator&lt;String&gt; applied = watermarkStream.keyBy(0)&#13;
                .timeWindow(Time.seconds(3))&#13;
                //.allowedLateness(Time.seconds(2))//允许数据迟到2秒&#13;
                .sideOutputLateData(outputTag)&#13;
                .apply(new WindowFunction&lt;Tuple2&lt;String, Long&gt;, String, Tuple, TimeWindow&gt;() {&#13;
                    @Override&#13;
                    public void apply(Tuple tuple, TimeWindow window, Iterable&lt;Tuple2&lt;String, Long&gt;&gt; input, Collector&lt;String&gt; out) throws Exception {&#13;
                        String key = tuple.toString();&#13;
                        List&lt;Long&gt; arrarList = new ArrayList&lt;Long&gt;();&#13;
                        Iterator&lt;Tuple2&lt;String, Long&gt;&gt; it = input.iterator();&#13;
                        while (it.hasNext()) {&#13;
                            Tuple2&lt;String, Long&gt; next = it.next();&#13;
                            arrarList.add(next.f1);&#13;
                        }&#13;
                        Collections.sort(arrarList);&#13;
                        SimpleDateFormat sdf = new SimpleDateFormat('yyyy-MM-dd HH:mm:ss.SSS');&#13;
                        String result = key + ',' + arrarList.size() + ',' + sdf.format(arrarList.get(0)) + ',' + sdf.format(arrarList.get(arrarList.size() - 1))&#13;
                                + ',' + sdf.format(window.getStart()) + ',' + sdf.format(window.getEnd());&#13;
                        out.collect(result);&#13;
                    }&#13;
                });&#13;
&#13;
        //把迟到的数据暂时打印到控制台，实际中可以保存到其他存储介质中&#13;
        DataStream&lt;Tuple2&lt;String, Long&gt;&gt; sideOutput = applied.getSideOutput(outputTag);&#13;
        applied.print();&#13;
&#13;
        env.execute();&#13;
&#13;
    }&#13;
```&#13;
&#13;
&#13;
&gt; 输入数据&#13;
                  0001,1538359882000&#13;
 0001,1538359886000&#13;
0001,1538359892000&#13;
0001,1538359893000&#13;
0001,1538359894000&#13;
&#13;
&gt; 输出数据&#13;
     eventTime*************/ currentMaxTimestamp / Watermark &#13;
     2018-10-01 10:11:**22** / 2018-10-01 10:11:22 / 2018-10-01 10:11:12&#13;
    2018-10-01 10:11:26 / 2018-10-01 10:11:26 / 2018-10-01 10:11:16&#13;
    2018-10-01 10:11:32 / 2018-10-01 10:11:32 / 2018-10-01 10:11:22&#13;
    2018-10-01 10:11:33 / 2018-10-01 10:11:33 / 2018-10-01 10:11:23&#13;
    2018-10-01 10:11:34 / 2018-10-01 10:11:34 / 2018-10-01 10:11:**24**&#13;
   &#13;
**设置的.timeWindow(Time.seconds(3))是系统划分的时间段,遵循前包后不包原则.**&#13;
**当Watermark&gt;=窗口的结束时间就触发并且窗口内有数据.**&#13;
## 对于迟到数据处理&#13;
1. 丢弃&#13;
2. 使用.allowedLateness(Time.seconds(2))允许数据迟到2秒(会对时间段内数据进行重新处理耗费内存/计算资源)&#13;
1. 对迟到数据进行统一回收.sideOutputLateData(**outputTag**) &#13;
```        &#13;
 //把迟到的数据打印到控制台，实际中可以保存到其他存储介质中&#13;
OutputTag&lt;Tuple2&lt;String, Long&gt;&gt; outputTag = new OutputTag&lt;Tuple2&lt;String, Long&gt;&gt;('late-data'){};&#13;
&#13;
.timeWindow(Time.seconds(3)).sideOutputLateData(outputTag)&#13;
&#13;
 DataStream&lt;Tuple2&lt;String, Long&gt;&gt; sideOutput = applied.getSideOutput(outputTag);&#13;
&#13;
 applied.print();。</description><guid isPermaLink="true">https://0or1world.github.io/post/Flink-zhi-Watermarks.html</guid><pubDate>Tue, 02 Jul 2024 13:32:27 +0000</pubDate></item><item><title>FlinkBathSQLscala</title><link>https://0or1world.github.io/post/FlinkBathSQLscala.html</link><description>**读文本**&#13;
```&#13;
val env = ExecutionEnvironment.getExecutionEnvironment&#13;
&#13;
    val tableENV = TableEnvironment.getTableEnvironment(env)&#13;
    //读取文本数据&#13;
    val unit = env.readTextFile('J:\\idea2019\\works\\sjmyexam03\\ru.txt')&#13;
    //隐式转换&#13;
    import org.apache.flink.api.scala._&#13;
    import org.apache.flink.table.api.scala._&#13;
    //使用map将读取到的数据切割放入元组(将数据放入DataSet)&#13;
    val unit1:DataSet[(String,String,String)] = unit.map(&#13;
      x =&gt; {&#13;
        val strings = x.split(',')&#13;
&#13;
        (strings(0), strings(1), strings(2))&#13;
&#13;
      }&#13;
&#13;
    )&#13;
  //注册表                   //表名   //DataSet //字段名  &#13;
    tableENV.registerDataSet('stu',unit1,'ids,'name,'jk)&#13;
                              //sqlQuery&#13;
    val table = tableENV.sqlQuery(' select ids,name,jk from stu where ids = '1'')&#13;
&#13;
    tableENV.toDataSet[Row](table).print()&#13;
&#13;
&#13;
  }&#13;
```&#13;
**读数组**&#13;
```&#13;
     val env = ExecutionEnvironment.getExecutionEnvironment&#13;
&#13;
    val tableENV = TableEnvironment.getTableEnvironment(env)&#13;
&#13;
    import org.apache.flink.api.scala._&#13;
    import org.apache.flink.table.api.scala._&#13;
&#13;
    val tuples = List(&#13;
      ('u001', 'alex', 'jk'),&#13;
      ('u002', 'lucy', 'lolita'),&#13;
      ('u003', 'rose', 'jk')&#13;
    )&#13;
&#13;
    val tuple3 = List(&#13;
      ('u001', '女'),&#13;
      ('u002', '女'),&#13;
      ('u003', '男')&#13;
    )&#13;
&#13;
&#13;
&#13;
      //转换数据&#13;
    val unit = env.fromCollection(tuples)&#13;
&#13;
    val value = env.fromCollection(tuple3)&#13;
    //注册表&#13;
    tableENV.registerDataSet('pro',unit,'ids,'name,'likes)&#13;
&#13;
    tableENV.registerDataSet('my',value,'ids,'sex)&#13;
    //查询&#13;
    val table = tableENV.sqlQuery('select * from pro p left join my j on p.ids=j.ids ')&#13;
&#13;
&#13;
    tableENV.toDataSet[Row](table).print()。</description><guid isPermaLink="true">https://0or1world.github.io/post/FlinkBathSQLscala.html</guid><pubDate>Tue, 02 Jul 2024 13:31:39 +0000</pubDate></item><item><title>Flink之ApplyAndWindowFunction</title><link>https://0or1world.github.io/post/Flink-zhi-ApplyAndWindowFunction.html</link><description>**apply中的WindowFunction使用方法**&#13;
```&#13;
.keyby(0,1)&#13;
.timeWindow(Time.seconds(60))&#13;
                            输入tuple4                                    输出tuple6&#13;
.apply(new WindowFunction&lt;Tuple4&lt;String, String, Double, Long&gt;, Tuple6&lt;String, String, Double, Double, &#13;
 Tuple里面有keyby的数据 TimeWindow可以get出窗口开始和结束时间&#13;
Double, Long&gt;, Tuple, TimeWindow&gt;() {&#13;
                    @Override&#13;
                    具体的逻辑                                       此处是keyby进行分组后的一组数据&#13;
                    public void apply(Tuple tuple, TimeWindow window, Iterable&lt;Tuple4&lt;String, String, Double, Long&gt;&gt; input, Collector&lt;Tuple6&lt;String, String, Double, Double, Double, Long&gt;&gt; out) throws Exception {&#13;
                        将数据迭代出来进行逻辑处理&#13;
                        Iterator&lt;Tuple4&lt;String, String, Double, Long&gt;&gt; it = input.iterator();&#13;
                        List&lt;Tuple4&lt;String, String, Double, Long&gt;&gt; dataList = new ArrayList&lt;&gt;();&#13;
&#13;
                        Long count = 0L;&#13;
                        Double sum = 0.0;&#13;
                        while (it.hasNext()) {&#13;
                            Tuple4&lt;String, String, Double, Long&gt; next = it.next();&#13;
                            sum += next.f2;&#13;
                            count++;&#13;
                            dataList.add(next);&#13;
                        }&#13;
                        Collections.sort(dataList, new Comparator&lt;Tuple4&lt;String, String, Double, Long&gt;&gt;() {&#13;
                            @Override&#13;
                            public int compare(Tuple4&lt;String, String, Double, Long&gt; o1, Tuple4&lt;String, String, Double, Long&gt; o2) {&#13;
                                return o2.f2.compareTo(o1.f2);&#13;
                            }&#13;
                        });&#13;
&#13;
                        double avg = sum / count;&#13;
                        double max = dataList.get(0).f2;&#13;
                        double min = dataList.get(dataList.size() - 1).f2;&#13;
&#13;
                        String devId = tuple.getField(0);&#13;
                        String metric = tuple.getField(1);&#13;
                         将数据发射输出&#13;
                        out.collect(Tuple6.of(devId, metric, max, min, avg, window.getStart()));&#13;
&#13;
                    }&#13;
                });。</description><guid isPermaLink="true">https://0or1world.github.io/post/Flink-zhi-ApplyAndWindowFunction.html</guid><pubDate>Tue, 02 Jul 2024 13:30:37 +0000</pubDate></item><item><title>Flink之ListState</title><link>https://0or1world.github.io/post/Flink-zhi-ListState.html</link><description>**process内使用KeyedProcessFunction**&#13;
liststate可以设置检查点当程序在某时刻停止再启动会继续(记录偏移量)&#13;
```&#13;
keyBy('windowEnd').process(new KeyedProcessFunction&lt;Tuple, ItemCount, String&gt;() {&#13;
&#13;
            ListState&lt;ItemCount&gt; listState = null;&#13;
&#13;
&#13;
&#13;
            //3.定时器实现逻辑&#13;
            @Override&#13;
            public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;String&gt; out) throws Exception {&#13;
&#13;
                ArrayList&lt;ItemCount&gt; itemCounts = new ArrayList&lt;&gt;();&#13;
                //将listState内的数据取出&#13;
                for (ItemCount itemCount : listState.get()) {&#13;
&#13;
                    itemCounts.add(itemCount);&#13;
&#13;
                }&#13;
&#13;
                //排序&#13;
                Collections.sort(itemCounts, new Comparator&lt;ItemCount&gt;() {&#13;
                    @Override&#13;
                    public int compare(ItemCount o1, ItemCount o2) {&#13;
                        return o1.count.compareTo(o2.count);&#13;
                    }&#13;
                });&#13;
&#13;
                StringBuffer stringBuffer = new StringBuffer('时间 :'+sdt.format(itemCounts.get(0).windowEnd));&#13;
&#13;
                for (int i = 0;i&lt;itemCounts.size();i++){&#13;
&#13;
                    ItemCount itemCount = itemCounts.get(i);&#13;
&#13;
                    stringBuffer.append('商品ID :'+itemCount.itemID+' 点击量 :'+itemCount.count+'\n');&#13;
&#13;
&#13;
                }&#13;
                //发送出去&#13;
                out.collect(stringBuffer.toString());&#13;
                //清理list&#13;
                itemCounts.clear();&#13;
&#13;
&#13;
&#13;
            }&#13;
&#13;
            @Override&#13;
            //1.将ListStateDescriptor描述创建出来作为全局使用&#13;
            public void open(Configuration parameters) throws Exception {&#13;
&#13;
                ListStateDescriptor&lt;ItemCount&gt; jk = new ListStateDescriptor&lt;&gt;(&#13;
                        'jk',//名称&#13;
                        TypeInformation.of(new TypeHint&lt;ItemCount&gt;() {&#13;
                        }) //类型&#13;
&#13;
&#13;
                );&#13;
&#13;
                //创建出listState&#13;
                listState = getRuntimeContext().getListState(jk);&#13;
&#13;
&#13;
            }&#13;
&#13;
            @Override&#13;
            //2.将数据添加到listState&#13;
            public void processElement(ItemCount value, Context ctx, Collector&lt;String&gt; out) throws Exception {&#13;
&#13;
&#13;
                listState.add(value);&#13;
                //创建定时器&#13;
                ctx.timerService().registerEventTimeTimer(value.windowEnd + 1);&#13;
&#13;
            }&#13;
&#13;
        });。</description><guid isPermaLink="true">https://0or1world.github.io/post/Flink-zhi-ListState.html</guid><pubDate>Tue, 02 Jul 2024 13:29:40 +0000</pubDate></item><item><title>RDD与算子</title><link>https://0or1world.github.io/post/RDD-yu-suan-zi.html</link><description>## 什么是RDD&#13;
- RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。</description><guid isPermaLink="true">https://0or1world.github.io/post/RDD-yu-suan-zi.html</guid><pubDate>Tue, 02 Jul 2024 13:28:29 +0000</pubDate></item><item><title>spark运行流程</title><link>https://0or1world.github.io/post/spark-yun-xing-liu-cheng.html</link><description>### spark中基本概念&#13;
1. Application：表示你的应用程序&#13;
2. Driver：表示main()函数，创建SparkContext。</description><guid isPermaLink="true">https://0or1world.github.io/post/spark-yun-xing-liu-cheng.html</guid><pubDate>Tue, 02 Jul 2024 13:27:50 +0000</pubDate></item><item><title>spark开发调优</title><link>https://0or1world.github.io/post/spark-kai-fa-diao-you.html</link><description>## 1. 避免重复创建RDD&#13;
通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。</description><guid isPermaLink="true">https://0or1world.github.io/post/spark-kai-fa-diao-you.html</guid><pubDate>Tue, 02 Jul 2024 13:26:56 +0000</pubDate></item><item><title>hive数据倾斜原因解决方法</title><link>https://0or1world.github.io/post/hive-shu-ju-qing-xie-yuan-yin-jie-jue-fang-fa.html</link><description>### Hive倾斜之group by聚合倾斜&#13;
**原因：**&#13;
- 分组的维度过少，每个维度的值过多，导致处理某值的reduce耗时很久；&#13;
&#13;
- 对一些类型统计的时候某种类型的数据量特别多，其他的数据类型特别少。</description><guid isPermaLink="true">https://0or1world.github.io/post/hive-shu-ju-qing-xie-yuan-yin-jie-jue-fang-fa.html</guid><pubDate>Tue, 02 Jul 2024 13:26:02 +0000</pubDate></item><item><title>SPARK参数</title><link>https://0or1world.github.io/post/SPARK-can-shu.html</link><description>&#13;
&#13;
## Driver&#13;
&#13;
**spark.driver.cores**&#13;
&#13;
driver端分配的核数，默认为1，thriftserver是启动thriftserver服务的机器，资源充足的话可以尽量给多。</description><guid isPermaLink="true">https://0or1world.github.io/post/SPARK-can-shu.html</guid><pubDate>Tue, 02 Jul 2024 13:24:22 +0000</pubDate></item><item><title>hive语句大全</title><link>https://0or1world.github.io/post/hive-yu-ju-da-quan.html</link><description>hive数据库是hdfs上的文件夹，表也是文件夹，表里的数据是文件&#13;
hive建表&#13;
create table t_student(id string,name string,age int,classNo string)&#13;
row format delimited&#13;
fields terminated by ',';&#13;
&#13;
创建外部表&#13;
create external table t_a(id string,name string)&#13;
row format delimited&#13;
fields terminated by ','&#13;
location '/.../...'&#13;
外部表和内部表的区别，drop时内部表的hdfs数据一同删除，外部表的hdfs上的数据不删除&#13;
&#13;
create table t_b(id string,name string)&#13;
row format delimited&#13;
fields terminated by ','&#13;
&#13;
create table t_a(id string,name string)&#13;
row format delimited&#13;
fields terminated by ','&#13;
&#13;
--加载数据从hdfs上加载&#13;
load data inpath '/dataB/b.txt' into table t_a;&#13;
--加载数据从本地上加载&#13;
load data local inpath '/root/bb.txt' into table t_b;&#13;
&#13;
t_a&#13;
+---------+-----------+--+&#13;
| t_a.id | t_a.name |&#13;
+---------+-----------+--+&#13;
| a | 1 |&#13;
| b | 2 |&#13;
| c | 4 |&#13;
| d | 5 |&#13;
+---------+-----------+--+&#13;
&#13;
t_b&#13;
+---------+-----------+--+&#13;
| t_b.id | t_b.name |&#13;
+---------+-----------+--+&#13;
| a | xx |&#13;
| b | yy |&#13;
| d | zz |&#13;
| e | pp |&#13;
+---------+-----------+--+&#13;
&#13;
--笛卡尔积&#13;
select a.,b.&#13;
from t_a inner join t_b&#13;
&#13;
+---------+-----------+---------+-----------+--+&#13;
| t_a.id | t_a.name | t_b.id | t_b.name |&#13;
+---------+-----------+---------+-----------+--+&#13;
| a | 1 | a | xx |&#13;
| b | 2 | a | xx |&#13;
| c | 4 | a | xx |&#13;
| d | 5 | a | xx |&#13;
| a | 1 | b | yy |&#13;
| b | 2 | b | yy |&#13;
| c | 4 | b | yy |&#13;
| d | 5 | b | yy |&#13;
| a | 1 | d | zz |&#13;
| b | 2 | d | zz |&#13;
| c | 4 | d | zz |&#13;
| d | 5 | d | zz |&#13;
| a | 1 | e | pp |&#13;
| b | 2 | e | pp |&#13;
| c | 4 | e | pp |&#13;
| d | 5 | e | pp |&#13;
+---------+-----------+---------+-----------+--+&#13;
&#13;
--内连接&#13;
select * from t_a join t_b where t_a.id = t_b.id;&#13;
select * from t_a join t_b on t_a.id = t_b.id;&#13;
&#13;
--左外连接&#13;
select&#13;
a.,b.&#13;
from t_a a left join t_b b&#13;
&#13;
select&#13;
a.,b.&#13;
from t_a a left join t_b b on a.id = b.id&#13;
&#13;
+-------+---------+-------+---------+--+&#13;
| a.id | a.name | b.id | b.name |&#13;
+-------+---------+-------+---------+--+&#13;
| a | 1 | a | xx |&#13;
| b | 2 | b | yy |&#13;
| d | 5 | d | zz |&#13;
| NULL | NULL | e | pp |&#13;
+-------+---------+-------+---------+--+&#13;
&#13;
--右外连接&#13;
select&#13;
a.,b.&#13;
from t_a a right outer join t_b b&#13;
&#13;
select&#13;
a.,b.&#13;
from t_a a right outer join t_b b on a.id = b.id&#13;
+-------+---------+-------+---------+--+&#13;
| a.id | a.name | b.id | b.name |&#13;
+-------+---------+-------+---------+--+&#13;
| a | 1 | a | xx |&#13;
| b | 2 | b | yy |&#13;
| d | 5 | d | zz |&#13;
| NULL | NULL | e | pp |&#13;
+-------+---------+-------+---------+--+&#13;
&#13;
--全外连接&#13;
select&#13;
a.,b.&#13;
from t_a a full outer join t_b b on a.id = b.id&#13;
&#13;
+-------+---------+-------+---------+--+&#13;
| a.id | a.name | b.id | b.name |&#13;
+-------+---------+-------+---------+--+&#13;
| a | 1 | a | xx |&#13;
| b | 2 | b | yy |&#13;
| c | 4 | NULL | NULL |&#13;
| d | 5 | d | zz |&#13;
| NULL | NULL | e | pp |&#13;
+-------+---------+-------+---------+--+&#13;
&#13;
--左半连接 相当于指定条件的内连接，但是只显示左边的表数据&#13;
select&#13;
a.*&#13;
from t_a a left semi join t_b b on a.id = b.id&#13;
&#13;
+-------+---------+--+&#13;
| a.id | a.name |&#13;
+-------+---------+--+&#13;
| a | 1 |&#13;
| b | 2 |&#13;
| d | 5 |&#13;
+-------+---------+--+&#13;
&#13;
--例如有这些数据&#13;
vi access.log.0804&#13;
192.168.33.3,http://www.sina.com/stu,2017-08-04 15:30:20&#13;
192.168.33.3,http://www.sina.com/teach,2017-08-04 15:35:20&#13;
192.168.33.4,http://www.sina.com/stu,2017-08-04 15:30:20&#13;
192.168.33.4,http://www.sina.com/job,2017-08-04 16:30:20&#13;
192.168.33.5,http://www.sina.com/job,2017-08-04 15:40:20&#13;
&#13;
vi access.log.0805&#13;
192.168.33.3,http://www.sina.com/stu,2017-08-05 15:30:20&#13;
192.168.44.3,http://www.sina.com/teach,2017-08-05 15:35:20&#13;
192.168.33.44,http://www.sina.com/stu,2017-08-05 15:30:20&#13;
192.168.33.46,http://www.sina.com/job,2017-08-05 16:30:20&#13;
192.168.33.55,http://www.sina.com/job,2017-08-05 15:40:20&#13;
&#13;
vi access.log.0806&#13;
192.168.133.3,http://www.sina.com/register,2017-08-06 15:30:20&#13;
192.168.111.3,http://www.sina.com/register,2017-08-06 15:35:20&#13;
192.168.34.44,http://www.sina.com/pay,2017-08-06 15:30:20&#13;
192.168.33.46,http://www.sina.com/excersize,2017-08-06 16:30:20&#13;
192.168.33.55,http://www.sina.com/job,2017-08-06 15:40:20&#13;
192.168.33.46,http://www.sina.com/excersize,2017-08-06 16:30:20&#13;
192.168.33.25,http://www.sina.com/job,2017-08-06 15:40:20&#13;
192.168.33.36,http://www.sina.com/excersize,2017-08-06 16:30:20&#13;
192.168.33.55,http://www.sina.com/job,2017-08-06 15:40:20&#13;
&#13;
--建分区表&#13;
create table t_access(ip string,url string,access_time string)&#13;
partitioned by (day string)&#13;
row format delimited&#13;
fields terminated by ','&#13;
&#13;
--导入分区表数据&#13;
load data local inpath '/root/access.log.0806' into table t_access partition(day='2017-08-06');&#13;
&#13;
--查看当前分区情况&#13;
show partitions t_access;&#13;
&#13;
--求每条url访问的次数&#13;
select&#13;
url,count(1)&#13;
from t_access&#13;
group by url;&#13;
&#13;
--求每条url访问的ip中，最大的ip是哪个&#13;
select&#13;
url,max(ip)&#13;
from t_access&#13;
group by url;&#13;
&#13;
--求每个ip访问同一个页面的记录中，最晚的一条&#13;
select&#13;
ip,url,max(access_time)&#13;
from t_access&#13;
group by ip,url&#13;
&#13;
--求PV&#13;
select&#13;
url,count(1)&#13;
from t_access&#13;
group by url&#13;
&#13;
+--------------------------------+------+--+&#13;
| url | _c1 |&#13;
+--------------------------------+------+--+&#13;
| http://www.sina.com/excersize | 3 |&#13;
| http://www.sina.com/job | 7 |&#13;
| http://www.sina.com/pay | 1 |&#13;
| http://www.sina.com/register | 2 |&#13;
| http://www.sina.com/stu | 4 |&#13;
| http://www.sina.com/teach | 2 |&#13;
+--------------------------------+------+--+&#13;
&#13;
--求UV&#13;
select&#13;
url,count(distinct(ip))&#13;
from t_access&#13;
group by url&#13;
&#13;
+--------------------------------+-----+--+&#13;
| url | c1 |&#13;
+--------------------------------+-----+--+&#13;
| http://www.sina.com/excersize | 2 |&#13;
| http://www.sina.com/job | 5 |&#13;
| http://www.sina.com/pay | 1 |&#13;
| http://www.sina.com/register | 2 |&#13;
| http://www.sina.com/stu | 3 |&#13;
| http://www.sina.com/teach | 2 |&#13;
+--------------------------------+-----+--+&#13;
&#13;
--求每条url访问的ip中，最大的ip是哪个&#13;
--PV&#13;
--UV&#13;
--用mapreduce实现&#13;
&#13;
--求8月6号pv，uv&#13;
select&#13;
count(1),url&#13;
from t_access&#13;
where day = '2017-08-06'&#13;
group by url&#13;
&#13;
--求8月4号以后，每天访问http://www.sina.com/job 的次数，以及访问者中ip最大的&#13;
select&#13;
count(1),max(ip),day&#13;
from t_access&#13;
where day &gt; '2017-08-04'&#13;
and url = 'http://www.sina.com/job'&#13;
group by day&#13;
&#13;
select&#13;
count(1),max(ip),day&#13;
from t_access&#13;
where url = 'http://www.sina.com/job'&#13;
group by day having day &gt; '2017-08-04'&#13;
&#13;
--求8月4号以后，每天每个页面访问总次数，以及页面最大的ip，并且上述查询结果中访问次数大于2的&#13;
select tmp.* from&#13;
(select&#13;
url,day,count(1) cnts,max(ip) ip&#13;
from t_access&#13;
group by day,url having day &gt; '2017-08-04') tmp&#13;
where tmp.cnts &gt; 2&#13;
&#13;
--每天，pv排序&#13;
select&#13;
url,day,count(1) cnts&#13;
from t_access&#13;
group by day,url&#13;
order by cnts desc&#13;
&#13;
--hive里的order by是全局排序&#13;
--sort by是在map里进行排序&#13;
&#13;
select&#13;
tmp.*&#13;
from&#13;
(select&#13;
url,day,count(1) cnts&#13;
from t_access&#13;
group by day,url) tmp&#13;
distribute by tmp.day sort by tmp.cnts desc&#13;
&#13;
--有如下数据&#13;
+-------------+---------------+--+&#13;
| t_sale.mid | t_sale.money |&#13;
+-------------+---------------+--+&#13;
| AA | 15.0 |&#13;
| AA | 20.0 |&#13;
| BB | 22.0 |&#13;
| CC | 44.0 |&#13;
+-------------+---------------+--+&#13;
&#13;
--distribute by先把数据分发到各个reduce中，然后sort by在各个reduce中进行局部排序&#13;
select&#13;
mid,money&#13;
from t_sale&#13;
distribute by mid sort by money desc&#13;
&#13;
+------+--------+--+&#13;
| mid | money |&#13;
+------+--------+--+&#13;
| AA | 15.0 |&#13;
| AA | 20.0 |&#13;
| BB | 22.0 |&#13;
| CC | 44.0 |&#13;
+------+--------+--+&#13;
&#13;
--cluster by mid 等于 distribute by mid sort by mid&#13;
--cluster by后面不能跟desc，asc，默认的只能升序&#13;
&#13;
--order by 是全排序，所有的数据会发送给一个reduceTask进行处理，在数据量大的时候，reduce就会超负荷&#13;
select&#13;
mid,money&#13;
from t_sale&#13;
order by money desc&#13;
&#13;
--设置最大的reduce启动个数&#13;
set hive.exec.reducers.max=10;&#13;
--设置reduce的启动个数&#13;
set mapreduce.job.reduce=3&#13;
&#13;
--hive数据类型&#13;
--数字类型&#13;
tinyint 1byte -128到127&#13;
smallint 2byte -32768到32767 char&#13;
int&#13;
bigint&#13;
float&#13;
double&#13;
&#13;
--日期类型&#13;
timestamp&#13;
date&#13;
&#13;
--字符串类型&#13;
string&#13;
varchar&#13;
char&#13;
&#13;
--混杂类型&#13;
boolean&#13;
binary 二进制&#13;
&#13;
--复合类型&#13;
--数组&#13;
&#13;
流浪地球,吴京:吴孟达:张飞:赵云,2019-09-11&#13;
我和我的祖国,葛优:黄渤:宋佳:吴京,2019-10-01&#13;
&#13;
create table t_movie(movie_name string,actors array&lt;string&gt;,show_time date)&#13;
row format delimited fields terminated by ','&#13;
collection items terminated by ':';&#13;
&#13;
array_contains&#13;
&#13;
select&#13;
&#13;
from t_movie&#13;
where array_contains(actors,'张飞')&#13;
&#13;
size&#13;
&#13;
select&#13;
movie_name,actors,show_time,size(actors) num&#13;
from t_movie&#13;
&#13;
========================================&#13;
&#13;
map类型&#13;
1,zhangsan,father:xiaoming#mother:chentianxing#brother:shaoshuai,28&#13;
2,xiaoming,father:xiaoyun#mother:xiaohong#sister:xiaoyue#wife:chentianxing,30&#13;
&#13;
create table t_user(id int,name string,family map&lt;string,string&gt;,jage int)&#13;
row format delimited fields terminated by ','&#13;
collection items terminated by '#'&#13;
map keys terminated by ':';&#13;
&#13;
map_keys&#13;
select&#13;
id,name,map_keys(family) as relation,jage&#13;
from t_user;&#13;
&#13;
map_values&#13;
select&#13;
id,name,map_values(family) as relation,jage&#13;
from t_user;&#13;
&#13;
--家庭成员数量&#13;
select&#13;
id,name,size(family),jage&#13;
from t_user&#13;
&#13;
--有兄弟的人&#13;
select&#13;
&#13;
from t_user&#13;
where array_contains(map_keys(family),'brother')&#13;
&#13;
========================================&#13;
1,zhangsan,18:man:beijing&#13;
2,lisi,22:woman:shanghai&#13;
&#13;
create table t_people(id int,name string,info struct&lt;age:int,sex:string,addr:string&gt;)&#13;
row format delimited fields terminated by ','&#13;
collection items terminated by ':';&#13;
&#13;
select&#13;
id,name,info.age&#13;
from t_people;&#13;
&#13;
========================================&#13;
&#13;
192,168,33,66,zhangsan,male&#13;
192,168,33,77,lisi,male&#13;
192,168,43,101,wangwu,female&#13;
&#13;
create table t_people_ip(ip_seg1 string,ip_seg2 string,ip_seg3 string,ip_seg4 string,name string,sex string)&#13;
row format delimited fields terminated by ',';&#13;
&#13;
--字符串拼接函数&#13;
concat&#13;
select concat('abc','def')&#13;
&#13;
concat_ws&#13;
select concat_ws('.',ip_seg1,ip_seg2,ip_seg3,ip_seg4),name,sex from t_people_ip&#13;
&#13;
===================================================================&#13;
&#13;
--求字符串长度&#13;
length&#13;
select lenth('jsdfijsdkfjkdsfjkdf');&#13;
&#13;
========================================&#13;
&#13;
select to_date('2019-09-11 16:55:11');&#13;
&#13;
--把字符串转换成unix时间戳&#13;
select unix_timestamp('2019-09-11 11:55:11','yyyy-MM-dd HH:mm:ss');&#13;
&#13;
--把unix时间戳转换成字符串&#13;
select from_unixtime(unix_timestamp(),'yyyy-MM-dd HH:mm:ss');&#13;
&#13;
========================================&#13;
&#13;
--数学运算函数&#13;
--四舍五入&#13;
select round(5.4)&#13;
--四舍五入保留2位小数&#13;
select round(5.1345,2)&#13;
--向上取整&#13;
select ceil(5.3)&#13;
--向下取整&#13;
select floor(5.3)&#13;
--取绝对值&#13;
select abs(-5.2)&#13;
--取最大值&#13;
select greatest(3,4,5,6,7)&#13;
--取最小值&#13;
select least(3,4,5,6,7)&#13;
&#13;
==================================================&#13;
&#13;
1,19,a,male&#13;
2,19,b,male&#13;
3,22,c,female&#13;
4,16,d,female&#13;
5,30,e,male&#13;
6,26,f,female&#13;
&#13;
+---------------+----------------+-----------------+----------------+--+&#13;
| t_student.id | t_student.age | t_student.name | t_student.sex |&#13;
+---------------+----------------+-----------------+----------------+--+&#13;
| 1 | 19 | a | male |&#13;
| 2 | 19 | b | male |&#13;
| 5 | 30 | e | male |&#13;
&#13;
| 3 | 22 | c | female |&#13;
| 4 | 16 | d | female |&#13;
| 6 | 26 | f | female |&#13;
&#13;
+---------------+----------------+-----------------+----------------+--+&#13;
&#13;
row_number() over()&#13;
&#13;
select&#13;
id,age,name,sex,row_number() over(partition by sex order by age desc) rk&#13;
from t_student&#13;
&#13;
select&#13;
tmp.*&#13;
from&#13;
(select&#13;
id,age,name,sex,row_number() over(partition by sex order by age desc) rk&#13;
from t_student) tmp&#13;
where tmp.rk = 1&#13;
&#13;
select&#13;
id,age,name,sex,rank() over(partition by sex order by age desc) rk&#13;
from t_student&#13;
&#13;
select&#13;
id,age,name,sex,dense_rank() over(partition by sex order by age desc) rk&#13;
from t_student&#13;
&#13;
========================================&#13;
&#13;
A,2015-01,5&#13;
A,2015-01,15&#13;
B,2015-01,5&#13;
A,2015-01,8&#13;
B,2015-01,25&#13;
A,2015-01,5&#13;
C,2015-01,10&#13;
C,2015-01,20&#13;
A,2015-02,4&#13;
A,2015-02,6&#13;
C,2015-02,30&#13;
C,2015-02,10&#13;
B,2015-02,10&#13;
B,2015-02,5&#13;
A,2015-03,14&#13;
A,2015-03,6&#13;
B,2015-03,20&#13;
B,2015-03,25&#13;
C,2015-03,10&#13;
C,2015-03,20&#13;
&#13;
--求每个用户每个月的销售额和到当月位置的累计销售额&#13;
create table t_saller(name string,month string,amount int)&#13;
row format delimited fields terminated by ','&#13;
&#13;
create table t_accumulate&#13;
as&#13;
select name,month,sum(amount) samount&#13;
from t_saller&#13;
group by name,month&#13;
&#13;
+--------------------+---------------------+-----------------------+--+&#13;
| t_accumulate.name | t_accumulate.month | t_accumulate.samount |&#13;
+--------------------+---------------------+-----------------------+--+&#13;
| A | 2015-01 | 33 |&#13;
| A | 2015-02 | 10 |&#13;
| A | 2015-03 | 20 |&#13;
| B | 2015-01 | 30 |&#13;
| B | 2015-02 | 15 |&#13;
| B | 2015-03 | 45 |&#13;
| C | 2015-01 | 30 |&#13;
| C | 2015-02 | 40 |&#13;
| C | 2015-03 | 30 |&#13;
+--------------------+---------------------+-----------------------+--+&#13;
--最前面一行&#13;
select&#13;
name,month,samount,sum(samount) over(partition by name order by month rows between unbounded preceding and current row) accumlateAmount&#13;
from&#13;
t_accumulate&#13;
&#13;
| A | 2015-01 | 33 |&#13;
| A | 2015-02 | 10 |&#13;
| A | 2015-03 | 20 |&#13;
| A | 2015-04 | 30 |&#13;
| A | 2015-05 | 15 |&#13;
| A | 2015-06 | 45 |&#13;
| A | 2015-07 | 30 |&#13;
| A | 2015-08 | 40 |&#13;
| A | 2015-09 | 30 |&#13;
&#13;
select&#13;
name,month,samount,sum(samount) over(partition by name order by month rows between 2 preceding and 1 following ) accumlateAmount&#13;
from&#13;
t_accumulate&#13;
&#13;
preceding |&#13;
当前行 | 窗口长度&#13;
following |&#13;
&#13;
min() over() ,max() over() , avg() over()&#13;
&#13;
========================================&#13;
&#13;
1,zhangsan,化学:物理:数学:语文&#13;
2,lisi,化学:数学:生物:生理:卫生&#13;
3,wangwu,化学:语文:英语:体育:生物&#13;
&#13;
create table t_stu_subject(id int,name string,subjects array&lt;string&gt;)&#13;
row format delimited fields terminated by ','&#13;
collection items terminated by ':'&#13;
&#13;
explode()&#13;
&#13;
select explode(subjects) from t_stu_subject;&#13;
&#13;
select distinct tmp.subs from (select explode(subjects) subs from t_stu_subject) tmp&#13;
&#13;
========================================&#13;
&#13;
lateral view连接函数&#13;
&#13;
select&#13;
id,name,sub&#13;
from&#13;
t_stu_subject lateral view explode(subjects) tmp as sub&#13;
&#13;
+-----+-----------+------+--+&#13;
| id | name | sub |&#13;
+-----+-----------+------+--+&#13;
| 1 | zhangsan | 化学 |&#13;
| 1 | zhangsan | 物理 |&#13;
| 1 | zhangsan | 数学 |&#13;
| 1 | zhangsan | 语文 |&#13;
| 2 | lisi | 化学 |&#13;
| 2 | lisi | 数学 |&#13;
| 2 | lisi | 生物 |&#13;
| 2 | lisi | 生理 |&#13;
| 2 | lisi | 卫生 |&#13;
| 3 | wangwu | 化学 |&#13;
| 3 | wangwu | 语文 |&#13;
| 3 | wangwu | 英语 |&#13;
| 3 | wangwu | 体育 |&#13;
| 3 | wangwu | 生物 |&#13;
+-----+-----------+------+--+&#13;
&#13;
========================================&#13;
&#13;
wordcount&#13;
&#13;
words&#13;
&#13;
create table words(line string)&#13;
&#13;
line&#13;
hello world hi tom and jack&#13;
hello chentianxing qiaoyuan and shaoshuai&#13;
hello hello hi tom and shaoshuai&#13;
chentianxing love saoshuai&#13;
hello love what is love how love&#13;
&#13;
split()&#13;
&#13;
select&#13;
tmp.word,count(1) cnts&#13;
from&#13;
(select&#13;
explode(split(line,' ')) word&#13;
from words) tmp&#13;
group by tmp.word order by cnts desc&#13;
&#13;
========================================&#13;
&#13;
--炸map&#13;
select&#13;
id,name,key,value&#13;
from&#13;
t_user&#13;
lateral view explode(family) tmp as key,value&#13;
&#13;
========================================&#13;
&#13;
--有web系统，每天产生下列的日志文件&#13;
&#13;
2017-09-15号的数据：&#13;
192.168.33.6,hunter,2017-09-15 10:30:20,/a&#13;
192.168.33.7,hunter,2017-09-15 10:30:26,/b&#13;
192.168.33.6,jack,2017-09-15 10:30:27,/a&#13;
192.168.33.8,tom,2017-09-15 10:30:28,/b&#13;
192.168.33.9,rose,2017-09-15 10:30:30,/b&#13;
192.168.33.10,julia,2017-09-15 10:30:40,/c&#13;
&#13;
2017-09-16号的数据：&#13;
192.168.33.16,hunter,2017-09-16 10:30:20,/a&#13;
192.168.33.18,jerry,2017-09-16 10:30:30,/b&#13;
192.168.33.26,jack,2017-09-16 10:30:40,/a&#13;
192.168.33.18,polo,2017-09-16 10:30:50,/b&#13;
192.168.33.39,nissan,2017-09-16 10:30:53,/b&#13;
192.168.33.39,nissan,2017-09-16 10:30:55,/a&#13;
192.168.33.39,nissan,2017-09-16 10:30:58,/c&#13;
192.168.33.20,ford,2017-09-16 10:30:54,/c&#13;
&#13;
2017-09-17号的数据：&#13;
192.168.33.46,hunter,2017-09-17 10:30:21,/a&#13;
192.168.43.18,jerry,2017-09-17 10:30:22,/b&#13;
192.168.43.26,tom,2017-09-17 10:30:23,/a&#13;
192.168.53.18,bmw,2017-09-17 10:30:24,/b&#13;
192.168.63.39,benz,2017-09-17 10:30:25,/b&#13;
192.168.33.25,haval,2017-09-17 10:30:30,/c&#13;
192.168.33.10,julia,2017-09-17 10:30:40,/c&#13;
&#13;
--统计日活跃用户&#13;
&#13;
--统计每日新增用户&#13;
&#13;
--统计历史用户&#13;
&#13;
--创建日志表&#13;
create table t_web_log(ip string,uname string,access_time string,url string)&#13;
partitioned by (day string)&#13;
row format delimited fields terminated by ',';&#13;
&#13;
--创建日活跃用户表&#13;
create table t_user_active_day&#13;
like&#13;
t_web_log;&#13;
&#13;
--统计日活跃用户&#13;
insert into table t_user_active_day partition(day='2017-09-15')&#13;
select tmp.ip,tmp.uname,tmp.access_time,tmp.url&#13;
from&#13;
(select&#13;
ip,uname,access_time,url,row_number() over(partition by uname order by access_time) rn&#13;
from&#13;
t_web_log where day='2017-09-15') tmp&#13;
where rn &lt;2&#13;
&#13;
--创建历史用户表&#13;
create table t_user_history(uname string)&#13;
&#13;
--创建日新增用户表&#13;
create table t_user_new_day like t_user_active_day;&#13;
&#13;
--统计日新增用户&#13;
insert into table t_user_new_day partition(day='2017-09-15')&#13;
select&#13;
tua.ip,tua.uname,tua.access_time,tua.url&#13;
from t_user_active_day tua&#13;
left join t_user_history tuh on tua.uname = tuh.uname&#13;
where tua.day = '2017-09-15' and tuh.uname IS NULL&#13;
&#13;
--记录历史用户&#13;
insert into table t_user_history&#13;
select&#13;
uname&#13;
from t_user_new_day&#13;
where day='2017-09-15'&#13;
&#13;
insert into table t_user_active_day partition(day='2017-09-16')&#13;
select tmp.ip,tmp.uname,tmp.access_time,tmp.url&#13;
from&#13;
(select&#13;
ip,uname,access_time,url,row_number() over(partition by uname order by access_time) rn&#13;
from&#13;
t_web_log where day='2017-09-16') tmp&#13;
where rn &lt;2&#13;
&#13;
insert into table t_user_new_day partition(day='2017-09-16')&#13;
select&#13;
tua.ip,tua.uname,tua.access_time,tua.url&#13;
from t_user_active_day tua&#13;
left join t_user_history tuh on tua.uname = tuh.uname&#13;
where tua.day = '2017-09-16' and tuh.uname IS NULL&#13;
&#13;
insert into table t_user_history&#13;
select&#13;
uname&#13;
from t_user_new_day&#13;
where day='2017-09-16'&#13;
&#13;
--桶表&#13;
--创建桶表，按id分成三个桶&#13;
create table t1(id int,name string,age int)&#13;
clustered by(id) into 3 buckets&#13;
row format delimited fields terminated by ',';&#13;
&#13;
--桶表插入数据的方式&#13;
create table t1_1(id int,name string,age int)&#13;
row format delimited fields terminated by ',';&#13;
&#13;
set hive.enforce.bucketing=true;&#13;
set mapreduce.job.reduces=3;&#13;
&#13;
insert into t1 select * from t1_1;&#13;
&#13;
========================================&#13;
hive自定义函数&#13;
package com.bwie.hive.udf;&#13;
&#13;
import com.alibaba.fastjson.JSONObject;&#13;
import org.apache.hadoop.hive.ql.exec.UDF;&#13;
&#13;
public class JsonUDF extends UDF {&#13;
&#13;
public String evaluate(String json,String colum){&#13;
    JSONObject jsonObject = JSONObject.parseObject(json);&#13;
    String value = jsonObject.getString(colum);&#13;
    return value;&#13;
}&#13;
}&#13;
&#13;
打包上传&#13;
add jar /root/hivetest-1.0-SNAPSHOT.jar;&#13;
create temporary function getjson as 'com.bwie.hive.udf.JsonUDF';&#13;
&#13;
select&#13;
getjson(json,'movie') as movie,getjson(json,'rate') as rate,from_unixtime(CAST(getjson(json,'timeStamp') as BIGINT),'yyyy-MM-dd HH:mm:ss') as showtime,getjson(json,'uid') as uid&#13;
from t_ex_rat limit 10;&#13;
&#13;
--创建永久函数&#13;
方法一、&#13;
在hive-site.xml里添加jar包&#13;
&lt;property&gt;&#13;
&lt;name&gt;hive.aux.jars.path&lt;/name&gt;&#13;
&lt;value&gt;file:///usr/local/apache-hive-1.2.2-bin/lib/hivetest-1.0-SNAPSHOT.jar&lt;/value&gt;&#13;
&lt;/property&gt;&#13;
疑似jar包要放到lib下才能生效&#13;
create function getjson AS 'com.bwie.hive.udf.JsonUDF'&#13;
&#13;
方法二、&#13;
create function getjson as 'com.bwie.hive.udf.JsonUDF' using jar 'hdfs://hdp1703/lib/hivetest-1.0-SNAPSHOT.jar'&#13;
&#13;
drop function getjson&#13;
&#13;
========================================&#13;
&#13;
create table t_employee(id int,name string,age int)&#13;
partitioned by(state string,city string)&#13;
row format delimited fields terminated by ','&#13;
&#13;
create table t_employee_orgin(id int,name string,age int,state string,city string)&#13;
row format delimited fields terminated by ',';&#13;
&#13;
load data local inpath '/root/employ.txt' into table t_employee_orgin;&#13;
&#13;
在严格模式下，多个分区一定有一个分区是固定的&#13;
insert into t_employee&#13;
partition(state='china',city)&#13;
select id,name,age,city&#13;
from t_employee_orgin&#13;
where state='china'&#13;
&#13;
--非严格模式&#13;
set hive.exec.dynamic.partition.mode=nostrict;&#13;
insert into t_employee&#13;
partition(state,city)&#13;
select id,name,age,state,city&#13;
from t_employee_orgin&#13;
&#13;
========================================&#13;
&#13;
select to_date('2019-09-20 17:30:00')&#13;
&#13;
unix_timestamp('2019/09/20 17:30:00','yyyy/MM/dd HH:mm:ss')&#13;
&#13;
select from_unixtime(unix_timestamp('2019/09/20 17:30:00','yyyy/MM/dd HH:mm:ss'),'yyyy-MM-dd HH:mm:ss')。</description><guid isPermaLink="true">https://0or1world.github.io/post/hive-yu-ju-da-quan.html</guid><pubDate>Tue, 02 Jul 2024 13:05:47 +0000</pubDate></item></channel></rss>